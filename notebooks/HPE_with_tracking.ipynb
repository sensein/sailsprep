{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRR8swnGXbqd"
      },
      "outputs": [],
      "source": [
        "!pip install openmim\n",
        "!pip install git+https://github.com/jin-s13/xtcocoapi\n",
        "!pip install deepface mediapipe\n",
        "!pip install facenet_pytorch\n",
        "!pip install filterpy\n",
        "!pip install tqdm scikit-learn scikit-image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqkhIWgFXcXz"
      },
      "outputs": [],
      "source": [
        "!pip uninstall torch torchvision torchaudio -y\n",
        "!pip install torch==2.0.1+cu117 torchvision==0.15.2+cu117 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!mim install mmengine\n",
        "!pip install mmcv==2.0.1 -f https://download.openmmlab.com/mmcv/dist/cu118/torch2.0/index.html\n",
        "!mim install mmdet==3.2.0\n",
        "!git clone https://github.com/open-mmlab/mmpose.git\n",
        "%cd mmpose\n",
        "!pip install -e .\n",
        "!pip install \"numpy<2.0\"\n",
        "#restart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpppuPW3XcN3",
        "outputId": "0b035813-718a-4dd3-cfc9-3e475c3e198b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/mmpose\n"
          ]
        }
      ],
      "source": [
        "%cd mmpose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laIu8qxA0mkh",
        "outputId": "afea813f-f2ce-4acd-dbe6-29a326fa1d83"
      },
      "outputs": [],
      "source": [
        "import mmcv\n",
        "from mmcv import imread\n",
        "import mmengine\n",
        "from mmengine.registry import init_default_scope\n",
        "import numpy as np\n",
        "from deepface import DeepFace\n",
        "from collections import defaultdict\n",
        "import cv2\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import time\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from skimage.feature import local_binary_pattern\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from facenet_pytorch import MTCNN\n",
        "\n",
        "from mmpose.apis import inference_topdown\n",
        "from mmpose.apis import init_model as init_pose_estimator\n",
        "from mmpose.evaluation.functional import nms\n",
        "from mmpose.registry import VISUALIZERS\n",
        "from mmpose.structures import merge_data_samples\n",
        "\n",
        "from mmdet.apis import inference_detector, init_detector\n",
        "torch.cuda.set_device(0)\n",
        "device = 'cuda:0'\n",
        "\n",
        "#model initialization\n",
        "\n",
        "#det model\n",
        "det_config = 'projects/rtmpose/rtmdet/person/rtmdet_m_640-8xb32_coco-person.py'\n",
        "det_checkpoint = 'https://download.openmmlab.com/mmpose/v1/projects/rtmpose/rtmdet_m_8xb32-100e_coco-obj365-person-235e8209.pth'\n",
        "\n",
        "#HPE model\n",
        "pose_config = 'configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_hrnet-w48_dark-8xb32-210e_coco-wholebody-384x288.py'\n",
        "pose_checkpoint = 'https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w48_coco_wholebody_384x288_dark-f5726563_20200918.pth'\n",
        "\n",
        "cfg_options = dict(model=dict(test_cfg=dict(output_heatmaps=True)))\n",
        "\n",
        "# Initialize models on GPU\n",
        "detector = init_detector(det_config, det_checkpoint, device=device)\n",
        "detector.to(device)\n",
        "\n",
        "pose_estimator = init_pose_estimator(\n",
        "    pose_config,\n",
        "    pose_checkpoint,\n",
        "    device=device,\n",
        "    cfg_options=cfg_options\n",
        ")\n",
        "pose_estimator.to(device)\n",
        "\n",
        "# pose visualizer\n",
        "pose_estimator.cfg.visualizer.radius = 3\n",
        "pose_estimator.cfg.visualizer.line_width = 1\n",
        "visualizer = VISUALIZERS.build(pose_estimator.cfg.visualizer)\n",
        "visualizer.set_dataset_meta(pose_estimator.dataset_meta)\n",
        "\n",
        "class PersonTracker:\n",
        "    def __init__(self, fps=30):\n",
        "        self.next_person_id = 1\n",
        "        self.active_tracks = {}  # Currently visible persons\n",
        "        self.lost_tracks = {}    # Recently lost persons (for re-ID)\n",
        "        self.track_history = {}  # Complete history for better re-identification\n",
        "\n",
        "        # Tracking thresholds\n",
        "        self.iou_threshold = 0.4\n",
        "        self.appearance_threshold = 0.6\n",
        "        self.face_threshold = 0.65\n",
        "\n",
        "        # Frame management\n",
        "        self.frame_count = 0\n",
        "        self.fps = fps\n",
        "        self.max_lost_frames = int(fps * 5)  # 5 seconds\n",
        "        self.max_retire_frames = int(fps * 120)  # 30 seconds before permanent deletion\n",
        "\n",
        "        # Feature weights\n",
        "        self.iou_weight = 0.2\n",
        "        self.appearance_weight = 0.6\n",
        "        self.face_weight = 0.2\n",
        "\n",
        "        # Person validation\n",
        "        self.min_person_height = 180\n",
        "        self.min_person_width = 60\n",
        "        self.max_aspect_ratio = 4.0\n",
        "\n",
        "        # Stability features\n",
        "        self.min_track_confidence = 0.6\n",
        "        self.track_history_length = 10\n",
        "        self.position_history_length = 20\n",
        "\n",
        "        # Motion prediction\n",
        "        self.use_motion_prediction = True\n",
        "        self.velocity_weight = 0.2\n",
        "\n",
        "        # ID management\n",
        "        self.max_reid_attempts = 3  # How many times to try re-identification\n",
        "        # Track confirmation\n",
        "        self.min_initial_frames = 5  # Require 5 consecutive detections before confirming track\n",
        "        self.confirmed_tracks = set()  # Tracks that have passed initialization\n",
        "\n",
        "    def _extract_features(self, person_img):\n",
        "        \"\"\"Enhanced feature extraction with more stable descriptors\"\"\"\n",
        "        try:\n",
        "            if person_img.size == 0:\n",
        "                return None\n",
        "\n",
        "            # Standardize size and convert color spaces\n",
        "            person_img = cv2.resize(person_img, (128, 256))  # larger size for better features\n",
        "            hsv = cv2.cvtColor(person_img, cv2.COLOR_BGR2HSV)\n",
        "            lab = cv2.cvtColor(person_img, cv2.COLOR_BGR2LAB)\n",
        "\n",
        "            # More comprehensive color histograms\n",
        "            h_hist = cv2.calcHist([hsv], [0], None, [16], [0, 180])\n",
        "            s_hist = cv2.calcHist([hsv], [1], None, [16], [0, 256])\n",
        "            l_hist = cv2.calcHist([lab], [0], None, [16], [0, 256])\n",
        "\n",
        "            # Normalize histograms\n",
        "            h_hist = cv2.normalize(h_hist, h_hist).flatten()\n",
        "            s_hist = cv2.normalize(s_hist, s_hist).flatten()\n",
        "            l_hist = cv2.normalize(l_hist, l_hist).flatten()\n",
        "\n",
        "            # Enhanced texture features\n",
        "            gray = cv2.cvtColor(person_img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "            # Multiple LBP configurations\n",
        "            lbp1 = local_binary_pattern(gray, 8, 1, method='uniform')\n",
        "            lbp2 = local_binary_pattern(gray, 16, 2, method='uniform')\n",
        "            lbp_hist1 = np.histogram(lbp1, bins=16, range=(0, 16))[0]\n",
        "            lbp_hist2 = np.histogram(lbp2, bins=16, range=(0, 16))[0]\n",
        "\n",
        "            # HOG features\n",
        "            hog = cv2.HOGDescriptor((128, 256), (16,16), (8,8), (8,8), 9)\n",
        "            hog_features = hog.compute(person_img).flatten()\n",
        "\n",
        "            # Combine features with more weighting on structural features\n",
        "            return np.concatenate([\n",
        "                h_hist * 0.5,\n",
        "                s_hist * 0.5,\n",
        "                l_hist * 0.5,\n",
        "                lbp_hist1 * 1.0,\n",
        "                lbp_hist2 * 1.0,\n",
        "                hog_features * 1.5\n",
        "            ])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Feature extraction error: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "    def _get_face_embedding(self, face_img):\n",
        "        \"\"\"Extract face embedding with quality checks\"\"\"\n",
        "        try:\n",
        "            if face_img.size == 0 or face_img.shape[0] < 40 or face_img.shape[1] < 40:\n",
        "                return None\n",
        "\n",
        "            # Enhance face quality\n",
        "            face_img = cv2.resize(face_img, (160, 160))\n",
        "            gray = cv2.cvtColor(face_img, cv2.COLOR_BGR2GRAY)\n",
        "            face_img = cv2.equalizeHist(gray)\n",
        "            face_img = cv2.cvtColor(face_img, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "            embedding = DeepFace.represent(face_img, model_name='Facenet', enforce_detection=False)\n",
        "            return np.array(embedding[0]['embedding'])\n",
        "        except Exception as e:\n",
        "            return None\n",
        "\n",
        "    def _predict_position(self, track):\n",
        "        \"\"\"Predict next position based on motion history\"\"\"\n",
        "        if len(track.get('position_history', [])) < 2:\n",
        "            return track['last_bbox']\n",
        "\n",
        "        positions = track['position_history']\n",
        "        recent_positions = positions[-3:]  # uses last 3 positions\n",
        "\n",
        "        # Calculate average velocity\n",
        "        velocities = []\n",
        "        for i in range(1, len(recent_positions)):\n",
        "            prev_center = self._get_bbox_center(recent_positions[i-1])\n",
        "            curr_center = self._get_bbox_center(recent_positions[i])\n",
        "            velocity = (curr_center[0] - prev_center[0], curr_center[1] - prev_center[1])\n",
        "            velocities.append(velocity)\n",
        "\n",
        "        if not velocities:\n",
        "            return track['last_bbox']\n",
        "\n",
        "        avg_velocity = (\n",
        "            sum(v[0] for v in velocities) / len(velocities),\n",
        "            sum(v[1] for v in velocities) / len(velocities)\n",
        "        )\n",
        "\n",
        "        # Predict next position\n",
        "        last_bbox = track['last_bbox']\n",
        "        last_center = self._get_bbox_center(last_bbox)\n",
        "        predicted_center = (\n",
        "            last_center[0] + avg_velocity[0],\n",
        "            last_center[1] + avg_velocity[1]\n",
        "        )\n",
        "\n",
        "        # Create predicted bbox\n",
        "        w, h = last_bbox[2] - last_bbox[0], last_bbox[3] - last_bbox[1]\n",
        "        predicted_bbox = [\n",
        "            predicted_center[0] - w/2,\n",
        "            predicted_center[1] - h/2,\n",
        "            predicted_center[0] + w/2,\n",
        "            predicted_center[1] + h/2\n",
        "        ]\n",
        "\n",
        "        return predicted_bbox\n",
        "\n",
        "    def _get_bbox_center(self, bbox):\n",
        "        \"\"\"Get center point of bounding box\"\"\"\n",
        "        return ((bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2)\n",
        "    def _calculate_similarity(self, detection, track):\n",
        "        \"\"\"Enhanced similarity scoring with more features\"\"\"\n",
        "        scores = {}\n",
        "\n",
        "        # 1. IoU score with predicted position\n",
        "        if self.use_motion_prediction and len(track.get('position_history', [])) >= 2:\n",
        "            predicted_bbox = self._predict_position(track)\n",
        "            iou_score = self._calculate_iou(detection['bbox'], predicted_bbox)\n",
        "            iou_score_last = self._calculate_iou(detection['bbox'], track['last_bbox'])\n",
        "            iou_score = max(iou_score, iou_score_last)\n",
        "        else:\n",
        "            iou_score = self._calculate_iou(detection['bbox'], track['last_bbox'])\n",
        "        scores['iou'] = iou_score\n",
        "\n",
        "        # 2. Appearance score (uses multiple historical features)\n",
        "        app_score = 0\n",
        "        if detection['features'] is not None and track['features']:\n",
        "            # Compare with multiple historical features\n",
        "            for feat in track['features'][-5:]:  # Last 5 features\n",
        "                score = self._compare_features(detection['features'], feat)\n",
        "                app_score = max(app_score, score * 0.9)  # slight decay for older features\n",
        "\n",
        "            #also compare with the first feature (for long-term consistency)\n",
        "            if len(track['features']) > 5:\n",
        "                first_score = self._compare_features(detection['features'], track['features'][0])\n",
        "                app_score = max(app_score, first_score * 0.7)\n",
        "        scores['appearance'] = app_score\n",
        "\n",
        "        # 3. Face score with quality checks\n",
        "        face_score = 0\n",
        "        if detection['face_embedding'] is not None and track['face_embeddings']:\n",
        "            # Compare with all face embeddings, weighted by recency\n",
        "            for i, face_emb in enumerate(track['face_embeddings'][-3:]):\n",
        "                score = self._compare_features(detection['face_embedding'], face_emb)\n",
        "                weight = 0.9 ** (len(track['face_embeddings'][-3:]) - i - 1)  # Recent faces weighted higher\n",
        "                face_score = max(face_score, score * weight)\n",
        "        scores['face'] = face_score\n",
        "\n",
        "        # 4. Motion consistency score\n",
        "        motion_score = 0\n",
        "        if len(track.get('position_history', [])) >= 2:\n",
        "            last_pos = track['position_history'][-1]\n",
        "            current_pos = detection['bbox']\n",
        "\n",
        "            # Calculate expected position based on velocity\n",
        "            if len(track['position_history']) >= 2:\n",
        "                prev_pos = track['position_history'][-2]\n",
        "                velocity = (\n",
        "                    (last_pos[0] - prev_pos[0]),\n",
        "                    (last_pos[1] - prev_pos[1])\n",
        "                )\n",
        "                expected_pos = (\n",
        "                    last_pos[0] + velocity[0],\n",
        "                    last_pos[1] + velocity[1],\n",
        "                    last_pos[2] + velocity[0],\n",
        "                    last_pos[3] + velocity[1]\n",
        "                )\n",
        "                motion_score = self._calculate_iou(current_pos, expected_pos)\n",
        "        scores['motion'] = motion_score\n",
        "\n",
        "        # 5. Size consistency score\n",
        "        size_score = 0\n",
        "        if len(track.get('position_history', [])) > 0:\n",
        "            last_bbox = track['last_bbox']\n",
        "            last_area = (last_bbox[2] - last_bbox[0]) * (last_bbox[3] - last_bbox[1])\n",
        "            curr_area = (detection['bbox'][2] - detection['bbox'][0]) * (detection['bbox'][3] - detection['bbox'][1])\n",
        "            size_ratio = min(last_area, curr_area) / (max(last_area, curr_area) + 1e-7)\n",
        "            size_score = size_ratio\n",
        "        scores['size'] = size_score\n",
        "\n",
        "        # Combined score with updated weights\n",
        "        combined_score = (\n",
        "            self.iou_weight * scores['iou'] +\n",
        "            self.appearance_weight * scores['appearance'] +\n",
        "            self.face_weight * scores['face'] +\n",
        "            0.15 * scores['motion'] +  # Motion consistency\n",
        "            0.1 * scores['size']      # Size consistency\n",
        "        )\n",
        "\n",
        "        # Apply temporal decay for lost tracks\n",
        "        if 'last_seen' in track and track['last_seen'] < self.frame_count:\n",
        "            frames_missing = self.frame_count - track['last_seen']\n",
        "            decay_factor = max(0.5, 1.0 - (frames_missing / self.max_lost_frames))\n",
        "            combined_score *= decay_factor\n",
        "\n",
        "        return combined_score, scores\n",
        "\n",
        "\n",
        "    def _match_detections_to_tracks(self, detections, track_dict, is_lost_tracks=False):\n",
        "        \"\"\"Improved matching with cascaded matching strategy\"\"\"\n",
        "        if not detections or not track_dict:\n",
        "            return {}\n",
        "\n",
        "        # First stage: Strong matches only\n",
        "        strong_matches = {}\n",
        "        track_ids = list(track_dict.keys())\n",
        "        cost_matrix = np.zeros((len(detections), len(track_ids)))\n",
        "\n",
        "        for det_idx, detection in enumerate(detections):\n",
        "            for track_idx, track_id in enumerate(track_ids):\n",
        "                track = track_dict[track_id]\n",
        "                combined_score, score_details = self._calculate_similarity(detection, track)\n",
        "\n",
        "                # Require strong appearance or face match\n",
        "                if (score_details['appearance'] > 0.7 or\n",
        "                    (score_details['face'] > 0.65 and score_details['face'] > 0)):\n",
        "                    cost_matrix[det_idx, track_idx] = 1.0 - combined_score\n",
        "\n",
        "        # Hungarian algorithm for strong matches\n",
        "        det_indices, track_indices = linear_sum_assignment(cost_matrix)\n",
        "        for det_idx, track_idx in zip(det_indices, track_indices):\n",
        "            cost = cost_matrix[det_idx, track_idx]\n",
        "            similarity = 1.0 - cost\n",
        "            if similarity >= 0.6:  # Higher threshold for strong matches\n",
        "                strong_matches[det_idx] = track_ids[track_idx]\n",
        "\n",
        "        # Second stage: Standard matching for remaining\n",
        "        remaining_detections = [i for i in range(len(detections)) if i not in strong_matches]\n",
        "        remaining_tracks = [tid for tid in track_ids if tid not in strong_matches.values()]\n",
        "\n",
        "        standard_matches = {}\n",
        "        if remaining_detections and remaining_tracks:\n",
        "            cost_matrix = np.zeros((len(remaining_detections), len(remaining_tracks)))\n",
        "\n",
        "            for det_idx, orig_det_idx in enumerate(remaining_detections):\n",
        "                detection = detections[orig_det_idx]\n",
        "                for track_idx, track_id in enumerate(remaining_tracks):\n",
        "                    track = track_dict[track_id]\n",
        "                    combined_score, _ = self._calculate_similarity(detection, track)\n",
        "                    cost_matrix[det_idx, track_idx] = 1.0 - combined_score\n",
        "\n",
        "            # Hungarian algorithm for standard matches\n",
        "            det_indices, track_indices = linear_sum_assignment(cost_matrix)\n",
        "            for det_idx, track_idx in zip(det_indices, track_indices):\n",
        "                cost = cost_matrix[det_idx, track_idx]\n",
        "                similarity = 1.0 - cost\n",
        "                min_threshold = 0.4 if is_lost_tracks else 0.5\n",
        "                if similarity >= min_threshold:\n",
        "                    orig_det_idx = remaining_detections[det_idx]\n",
        "                    standard_matches[orig_det_idx] = remaining_tracks[track_idx]\n",
        "\n",
        "        return {**strong_matches, **standard_matches}\n",
        "\n",
        "    def update(self, frame, pose_results, face_boxes):\n",
        "        \"\"\"Main tracking update\"\"\"\n",
        "        self.frame_count += 1\n",
        "\n",
        "        # Extract detections\n",
        "        detections = []\n",
        "        for pose_idx, pose in enumerate(pose_results):\n",
        "            if hasattr(pose.pred_instances, 'bboxes') and len(pose.pred_instances.bboxes) > 0:\n",
        "                bbox = pose.pred_instances.bboxes[0]\n",
        "                if hasattr(bbox, 'cpu'):\n",
        "                    bbox = bbox.cpu().numpy()\n",
        "                elif not isinstance(bbox, np.ndarray):\n",
        "                    bbox = np.array(bbox)\n",
        "\n",
        "                if not self._validate_detection(bbox):\n",
        "                    continue\n",
        "\n",
        "                # Extract person image and features\n",
        "                x1, y1, x2, y2 = bbox.astype(int)\n",
        "                x1, y1 = max(0, x1), max(0, y1)\n",
        "                x2, y2 = min(frame.shape[1], x2), min(frame.shape[0], y2)\n",
        "\n",
        "                if x2 > x1 and y2 > y1:\n",
        "                    person_img = frame[y1:y2, x1:x2]\n",
        "                    features = self._extract_features(person_img)\n",
        "\n",
        "                    # Face matching\n",
        "                    face_embedding = None\n",
        "                    best_face_overlap = 0\n",
        "\n",
        "                    for face in face_boxes:\n",
        "                        face_x, face_y, face_w, face_h = face['box']\n",
        "                        face_bbox = [face_x, face_y, face_x + face_w, face_y + face_h]\n",
        "                        overlap = self._calculate_iou(bbox, face_bbox)\n",
        "\n",
        "                        if overlap > best_face_overlap and overlap > 0.1:\n",
        "                            best_face_overlap = overlap\n",
        "                            face_embedding = self._get_face_embedding(face['img'])\n",
        "\n",
        "                    keypoints = pose.pred_instances.keypoints[0]\n",
        "                    if hasattr(keypoints, 'cpu'):\n",
        "                        keypoints = keypoints.cpu().numpy()\n",
        "\n",
        "                    detections.append({\n",
        "                        'pose_idx': pose_idx,\n",
        "                        'bbox': bbox,\n",
        "                        'features': features,\n",
        "                        'face_embedding': face_embedding,\n",
        "                        'keypoints': keypoints,\n",
        "                        'person_img': person_img\n",
        "                    })\n",
        "\n",
        "        # Match with active tracks first\n",
        "        active_matches = self._match_detections_to_tracks(detections, self.active_tracks)\n",
        "\n",
        "        # Match remaining detections with lost tracks (re-identification)\n",
        "        unmatched_detection_indices = [i for i in range(len(detections)) if i not in active_matches]\n",
        "        unmatched_detections = [detections[i] for i in unmatched_detection_indices]\n",
        "\n",
        "        lost_matches = {}\n",
        "        if unmatched_detections and self.lost_tracks:\n",
        "            # Try multiple times with different thresholds for better re-ID\n",
        "            for attempt in range(self.max_reid_attempts):\n",
        "                temp_threshold = 0.3 + (attempt * 0.05)  # Gradually relax threshold\n",
        "                temp_matches = self._match_detections_to_tracks(\n",
        "                    unmatched_detections,\n",
        "                    self.lost_tracks,\n",
        "                    is_lost_tracks=True\n",
        "                )\n",
        "\n",
        "                # Convert back to original indices\n",
        "                for unmatched_idx, track_id in temp_matches.items():\n",
        "                    original_det_idx = unmatched_detection_indices[unmatched_idx]\n",
        "                    lost_matches[original_det_idx] = track_id\n",
        "\n",
        "                # Remove matched detections from next attempt\n",
        "                unmatched_detection_indices = [i for i in unmatched_detection_indices if i not in lost_matches]\n",
        "                unmatched_detections = [detections[i] for i in unmatched_detection_indices]\n",
        "\n",
        "                if not unmatched_detections:\n",
        "                    break\n",
        "\n",
        "        all_matches = {**active_matches, **lost_matches}\n",
        "\n",
        "        # Update matched tracks\n",
        "        for det_idx, track_id in all_matches.items():\n",
        "            detection = detections[det_idx]\n",
        "\n",
        "            # Move from lost to active if necessary\n",
        "            if track_id in self.lost_tracks:\n",
        "                self.active_tracks[track_id] = self.lost_tracks.pop(track_id)\n",
        "                print(f\"Re-identified person {track_id} after {self.frame_count - self.active_tracks[track_id]['last_seen']} frames\")\n",
        "\n",
        "            track = self.active_tracks[track_id]\n",
        "\n",
        "            # Update track information\n",
        "            track['last_bbox'] = detection['bbox']\n",
        "            track['last_seen'] = self.frame_count\n",
        "            track['track_length'] += 1\n",
        "\n",
        "            # Update position history\n",
        "            if 'position_history' not in track:\n",
        "                track['position_history'] = []\n",
        "            track['position_history'].append(detection['bbox'])\n",
        "            if len(track['position_history']) > self.position_history_length:\n",
        "                track['position_history'].pop(0)\n",
        "\n",
        "            # Update features\n",
        "            if detection['features'] is not None:\n",
        "                if 'features' not in track:\n",
        "                    track['features'] = []\n",
        "                track['features'].append(detection['features'])\n",
        "                if len(track['features']) > self.track_history_length:\n",
        "                    track['features'].pop(0)\n",
        "\n",
        "            # Update face embeddings\n",
        "            if detection['face_embedding'] is not None:\n",
        "                if 'face_embeddings' not in track:\n",
        "                    track['face_embeddings'] = []\n",
        "                track['face_embeddings'].append(detection['face_embedding'])\n",
        "                if len(track['face_embeddings']) > 5:  # Keeping recent faces\n",
        "                    track['face_embeddings'].pop(0)\n",
        "\n",
        "        # create new tracks for unmatched detections\n",
        "        for det_idx, detection in enumerate(detections):\n",
        "            if det_idx not in all_matches:\n",
        "                person_id = self.next_person_id\n",
        "                self.next_person_id += 1\n",
        "\n",
        "                new_track = {\n",
        "                    'last_bbox': detection['bbox'],\n",
        "                    'last_seen': self.frame_count,\n",
        "                    'created_frame': self.frame_count,\n",
        "                    'track_length': 1,\n",
        "                    'features': [detection['features']] if detection['features'] is not None else [],\n",
        "                    'face_embeddings': [detection['face_embedding']] if detection['face_embedding'] is not None else [],\n",
        "                    'position_history': [detection['bbox']],\n",
        "                    'unconfirmed_frames': 1  # Track initialization counter\n",
        "                }\n",
        "\n",
        "                self.active_tracks[person_id] = new_track\n",
        "                all_matches[det_idx] = person_id\n",
        "                print(f\"New person detected with ID: {person_id}\")\n",
        "\n",
        "        # Move tracks to lost if they're no longer visible\n",
        "        currently_visible = set(all_matches.values())\n",
        "        newly_lost = [track_id for track_id in self.active_tracks.keys()\n",
        "                     if track_id not in currently_visible]\n",
        "\n",
        "        for track_id in newly_lost:\n",
        "            self.lost_tracks[track_id] = self.active_tracks.pop(track_id)\n",
        "            print(f\"Person {track_id} lost from view\")\n",
        "\n",
        "        # Clean up old lost tracks\n",
        "        self._cleanup_lost_tracks()\n",
        "\n",
        "        # Returns confirmed assignments\n",
        "        result_assignments = {}\n",
        "        for pose_idx, person_id in all_matches.items():\n",
        "            track = self.active_tracks.get(person_id, None)\n",
        "            if track:\n",
        "                if person_id in self.confirmed_tracks:\n",
        "                    result_assignments[pose_idx] = person_id\n",
        "                else:\n",
        "                    track['unconfirmed_frames'] += 1\n",
        "                    if track['unconfirmed_frames'] >= self.min_initial_frames:\n",
        "                        self.confirmed_tracks.add(person_id)\n",
        "                        result_assignments[pose_idx] = person_id\n",
        "\n",
        "        return result_assignments\n",
        "\n",
        "\n",
        "\n",
        "    def _cleanup_lost_tracks(self):\n",
        "        \"\"\"Remove tracks that have been lost too long\"\"\"\n",
        "        current_frame = self.frame_count\n",
        "        to_remove = []\n",
        "\n",
        "        for track_id, track in self.lost_tracks.items():\n",
        "            frames_lost = current_frame - track['last_seen']\n",
        "\n",
        "            if frames_lost > self.max_lost_frames:\n",
        "                to_remove.append(track_id)\n",
        "\n",
        "        for track_id in to_remove:\n",
        "            del self.lost_tracks[track_id]\n",
        "            print(f\"Permanently removed person {track_id} from tracking\")\n",
        "\n",
        "    def _validate_detection(self, bbox):\n",
        "        \"\"\"Validate detection based on size and aspect ratio\"\"\"\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        width = x2 - x1\n",
        "        height = y2 - y1\n",
        "\n",
        "        return (height >= self.min_person_height and\n",
        "                width >= self.min_person_width and\n",
        "                width/height <= self.max_aspect_ratio)\n",
        "\n",
        "    def _calculate_iou(self, box1, box2):\n",
        "        \"\"\"Calculate Intersection over Union\"\"\"\n",
        "        x1_1, y1_1, x2_1, y2_1 = box1\n",
        "        x1_2, y1_2, x2_2, y2_2 = box2\n",
        "\n",
        "        x1_i = max(x1_1, x1_2)\n",
        "        y1_i = max(y1_1, y1_2)\n",
        "        x2_i = min(x2_1, x2_2)\n",
        "        y2_i = min(y2_1, y2_2)\n",
        "\n",
        "        if x2_i <= x1_i or y2_i <= y1_i:\n",
        "            return 0.0\n",
        "\n",
        "        intersection = (x2_i - x1_i) * (y2_i - y1_i)\n",
        "        area1 = (x2_1 - x1_1) * (y2_1 - y1_1)\n",
        "        area2 = (x2_2 - x1_2) * (y2_2 - y1_2)\n",
        "        union = area1 + area2 - intersection\n",
        "\n",
        "        return intersection / union if union > 0 else 0.0\n",
        "\n",
        "    def _compare_features(self, feat1, feat2):\n",
        "        \"\"\"Compare features using cosine similarity\"\"\"\n",
        "        if feat1 is None or feat2 is None:\n",
        "            return 0.0\n",
        "        try:\n",
        "            feat1 = np.array(feat1).reshape(1, -1)\n",
        "            feat2 = np.array(feat2).reshape(1, -1)\n",
        "            similarity = cosine_similarity(feat1, feat2)[0][0]\n",
        "            return max(0, similarity)\n",
        "        except:\n",
        "            return 0.0\n",
        "\n",
        "def detect_faces(frame):\n",
        "\n",
        "    mtcnn = MTCNN(keep_all=True, device=device)\n",
        "\n",
        "    # Convert to RGB\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    boxes, probs = mtcnn.detect(frame_rgb)\n",
        "\n",
        "    face_boxes = []\n",
        "    if boxes is not None:\n",
        "        for box, prob in zip(boxes, probs):\n",
        "            if prob > 0.9:  # high confidence threshold\n",
        "                x1, y1, x2, y2 = box.astype(int)\n",
        "                face_img = frame[y1:y2, x1:x2]\n",
        "                if face_img.size > 0:\n",
        "                    face_boxes.append({\n",
        "                        'box': [x1, y1, x2-x1, y2-y1],\n",
        "                        'img': face_img,\n",
        "                        'confidence': prob\n",
        "                    })\n",
        "    return face_boxes\n",
        "\n",
        "\n",
        "def visualize_frame(frame, pose_results, person_assignments, visualizer):\n",
        "    \"\"\"Visualize frame with pose estimation and person IDs - only for tracked persons\"\"\"\n",
        "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Filter pose results to only include tracked persons\n",
        "    filtered_pose_results = []\n",
        "    for pose_idx, pose in enumerate(pose_results):\n",
        "        if pose_idx in person_assignments:\n",
        "            filtered_pose_results.append(pose)\n",
        "\n",
        "    # If no tracked persons, return original frame\n",
        "    if not filtered_pose_results:\n",
        "        return frame\n",
        "\n",
        "    # Merge and visualize only the filtered poses\n",
        "    data_samples = merge_data_samples(filtered_pose_results)\n",
        "\n",
        "    visualizer.add_datasample(\n",
        "        'result',\n",
        "        img,\n",
        "        data_sample=data_samples,\n",
        "        draw_gt=False,\n",
        "        draw_heatmap=False,\n",
        "        draw_bbox=False,\n",
        "        show=False,\n",
        "        wait_time=0,\n",
        "        out_file=None,\n",
        "        kpt_thr=0.3\n",
        "    )\n",
        "\n",
        "    vis_result = visualizer.get_image()\n",
        "    vis_result = cv2.cvtColor(vis_result, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # Draw bounding boxes and IDs for tracked persons\n",
        "    for pose_idx, person_id in person_assignments.items():\n",
        "        if pose_idx < len(pose_results):\n",
        "            pose = pose_results[pose_idx]\n",
        "            if hasattr(pose.pred_instances, 'bboxes') and len(pose.pred_instances.bboxes) > 0:\n",
        "                bbox = pose.pred_instances.bboxes[0]\n",
        "                if torch.is_tensor(bbox):\n",
        "                    bbox = bbox.cpu().numpy()\n",
        "\n",
        "                x1, y1, x2, y2 = bbox.astype(int)\n",
        "\n",
        "                cv2.rectangle(vis_result, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "                text = f\"Person {person_id}\"\n",
        "                text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)[0]\n",
        "                cv2.rectangle(vis_result, (x1, y1-35), (x1+text_size[0]+10, y1-5), (0, 255, 0), -1)\n",
        "                cv2.putText(vis_result, text, (x1+5, y1-15),\n",
        "                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)\n",
        "\n",
        "    return vis_result\n",
        "\n",
        "def process_video(input_path, output_path, detector, pose_estimator, visualizer):\n",
        "    \"\"\"Process video with tracking\"\"\"\n",
        "    cap = cv2.VideoCapture(input_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Could not open video {input_path}\")\n",
        "        return\n",
        "\n",
        "    fps = 15\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    if not writer.isOpened():\n",
        "        print(f\"Error: Could not create video writer for {output_path}\")\n",
        "        cap.release()\n",
        "        return\n",
        "\n",
        "    tracker = PersonTracker(fps=fps)\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    with tqdm(total=frame_count, desc=\"Processing video\") as pbar:\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            if pbar.n % 100 == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            scope = detector.cfg.get('default_scope', 'mmdet')\n",
        "            if scope is not None:\n",
        "                init_default_scope(scope)\n",
        "\n",
        "            with torch.cuda.device(device):\n",
        "                detect_result = inference_detector(detector, frame)\n",
        "\n",
        "            pred_instance = detect_result.pred_instances.cpu().numpy()\n",
        "            bboxes = np.concatenate(\n",
        "                (pred_instance.bboxes, pred_instance.scores[:, None]), axis=1)\n",
        "            bboxes = bboxes[np.logical_and(pred_instance.labels == 0,\n",
        "                                        pred_instance.scores > 0.5)]\n",
        "            bboxes = bboxes[nms(bboxes, 0.7)][:, :4]\n",
        "\n",
        "            face_boxes = detect_faces(frame)\n",
        "\n",
        "            with torch.cuda.device(device):\n",
        "                pose_results = inference_topdown(pose_estimator, frame, bboxes)\n",
        "\n",
        "            person_assignments = tracker.update(frame, pose_results, face_boxes)\n",
        "\n",
        "            vis_frame = visualize_frame(frame, pose_results, person_assignments, visualizer)\n",
        "            writer.write(vis_frame)\n",
        "            pbar.update(1)\n",
        "\n",
        "    cap.release()\n",
        "    writer.release()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Main\n",
        "input_folder = '/content/video'\n",
        "output_folder = '/content/video_output'\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "video_files = [f for f in os.listdir(input_folder) if f.endswith('mp4')]\n",
        "\n",
        "print(f\"Found {len(video_files)} video(s).\")\n",
        "\n",
        "for video_file in video_files:\n",
        "    input_path = os.path.join(input_folder, video_file)\n",
        "    output_path = os.path.join(output_folder, video_file.replace('.mp4', '_tracked.mp4'))\n",
        "\n",
        "    print(f\"\\nProcessing: {video_file}\")\n",
        "    process_video(input_path, output_path, detector, pose_estimator, visualizer)\n",
        "    print(f\"Saved: {output_path}\")\n",
        "\n",
        "print(\"videos saved to output folder.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWvLGEcWTeIU"
      },
      "source": [
        "# Output and input BIDS format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbzApB22Gv9C"
      },
      "outputs": [],
      "source": [
        "\n",
        "import glob\n",
        "\n",
        "#BIDS format input\n",
        "input_folder = '/content/test_data/processed_output/bids-dataset/derivatives/preprocessed'  # Read from derivatives/preprocessed\n",
        "output_folder = '/content/bids-dataset/derivatives/tracked'  # Output to derivatives/tracked\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "\n",
        "\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Find all preprocessed video files\n",
        "video_pattern = os.path.join(input_folder, 'sub-*/ses-*/beh/*.mp4')\n",
        "video_files = glob.glob(video_pattern)\n",
        "\n",
        "print(f\"Found {len(video_files)} preprocessed video(s) in BIDS derivatives.\")\n",
        "\n",
        "for video_path in video_files:\n",
        "    # Example: /content/bids-dataset/derivatives/preprocessed/sub-01/ses-01/beh/sub-01_ses-01_task-play_desc-processed_beh.mp4\n",
        "    path_parts = video_path.split(os.sep)\n",
        "\n",
        "    # Find subject and session from path\n",
        "    subject = None\n",
        "    session = None\n",
        "    for part in path_parts:\n",
        "        if part.startswith('sub-'):\n",
        "            subject = part\n",
        "        elif part.startswith('ses-'):\n",
        "            session = part\n",
        "\n",
        "    video_filename = os.path.basename(video_path)\n",
        "\n",
        "    # Create output directory following BIDS derivatives structure\n",
        "    output_subject_dir = os.path.join(output_folder, subject, session, 'beh')\n",
        "    os.makedirs(output_subject_dir, exist_ok=True)\n",
        "\n",
        "    # Create output filename (replace desc-processed with desc-tracked)\n",
        "    if 'desc-processed' in video_filename:\n",
        "        output_filename = video_filename.replace('desc-processed', 'desc-tracked')\n",
        "    else:\n",
        "        # If no desc-processed, add desc-tracked before extension ( if we use raw data)\n",
        "        output_filename = video_filename.replace('.mp4', '_desc-tracked.mp4')\n",
        "    output_path = os.path.join(output_subject_dir, output_filename)\n",
        "\n",
        "    print(f\"\\nProcessing: {subject}/{session}/{video_filename}\")\n",
        "    print(f\"Input: {video_path}\")\n",
        "    print(f\"Output: {output_path}\")\n",
        "\n",
        "    process_video(video_path, output_path, detector, pose_estimator, visualizer)\n",
        "    print(f\"Saved: {output_path}\")\n",
        "\n",
        "print(\"All preprocessed videos processed and saved in BIDS derivatives/tracked structure.\")\n",
        "\n",
        "# create a output_description.json for the derivatives\n",
        "derivatives_description = {\n",
        "    \"Name\": \"Person Tracking with Pose Estimation\",\n",
        "    \"BIDSVersion\": \"1.10.0\",\n",
        "    \"GeneratedBy\": [\n",
        "        {\n",
        "            \"Name\": \"Custom Person Tracking Pipeline\",\n",
        "            \"Version\": \"1.0\",\n",
        "            \"Description\": \"Multi-person tracking with pose estimation using MMPose and face recognition\"\n",
        "        }\n",
        "    ],\n",
        "    \"SourceDatasets\": [\n",
        "        {\n",
        "            \"URL\": input_folder,\n",
        "            \"Version\": \"\"\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "import json\n",
        "description_path = os.path.join(output_folder, 'output_description.json')\n",
        "with open(description_path, 'w') as f:\n",
        "    json.dump(derivatives_description, f, indent=2)\n",
        "\n",
        "print(f\"Created dataset description: {description_path}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
