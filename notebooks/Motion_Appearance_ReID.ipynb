{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b523f9d7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install openmim\n",
    "!pip install git+https://github.com/jin-s13/xtcocoapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58369dc8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install deepface\n",
    "!pip install facenet_pytorch\n",
    "!pip install filterpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563b1928",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c39a3d5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip uninstall torch torchvision torchaudio -y\n",
    "!pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install --trusted-host download.openmmlab.com -f https://download.openmmlab.com/mmcv/dist/cu118/torch2.0.0/index.html mmcv==2.0.1\n",
    "!mim install mmengine\n",
    "!mim install mmdet==3.2.0\n",
    "!git clone https://github.com/open-mmlab/mmpose.git\n",
    "%cd mmpose\n",
    "!pip install -e .\n",
    "!pip install \"numpy<2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd512e6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a9fd85",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%cd mmpose "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d312d964",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import mmcv\n",
    "from mmcv import imread\n",
    "import mmengine\n",
    "from mmengine.registry import init_default_scope\n",
    "import numpy as np\n",
    "from deepface import DeepFace\n",
    "from collections import defaultdict, deque\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import time\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from facenet_pytorch import MTCNN\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import gc\n",
    "import math\n",
    "\n",
    "from mmpose.apis import inference_topdown\n",
    "from mmpose.apis import init_model as init_pose_estimator\n",
    "from mmpose.evaluation.functional import nms\n",
    "from mmpose.registry import VISUALIZERS\n",
    "from mmpose.structures import merge_data_samples\n",
    "\n",
    "from mmdet.apis import inference_detector, init_detector\n",
    "from filterpy.kalman import KalmanFilter\n",
    "\n",
    "\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "device = 'cuda:0'\n",
    "\n",
    "det_config = 'projects/rtmpose/rtmdet/person/rtmdet_m_640-8xb32_coco-person.py'\n",
    "det_checkpoint = 'https://download.openmmlab.com/mmpose/v1/projects/rtmpose/rtmdet_m_8xb32-100e_coco-obj365-person-235e8209.pth'\n",
    "pose_config = 'configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_hrnet-w48_dark-8xb32-210e_coco-wholebody-384x288.py'\n",
    "pose_checkpoint = 'https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w48_coco_wholebody_384x288_dark-f5726563_20200918.pth'\n",
    "\n",
    "\n",
    "detector = init_detector(det_config, det_checkpoint, device=device)\n",
    "pose_estimator = init_pose_estimator(pose_config, pose_checkpoint, device=device, \n",
    "                                    cfg_options=dict(model=dict(test_cfg=dict(output_heatmaps=True))))\n",
    "mtcnn = MTCNN(keep_all=True, device=device, post_process=False, min_face_size=40)\n",
    "\n",
    "pose_estimator.cfg.visualizer.radius = 3\n",
    "pose_estimator.cfg.visualizer.line_width = 1\n",
    "visualizer = VISUALIZERS.build(pose_estimator.cfg.visualizer)\n",
    "visualizer.set_dataset_meta(pose_estimator.dataset_meta)\n",
    "\n",
    "\n",
    "\n",
    "frame_count = 0\n",
    "next_track_id = 1\n",
    "active_tracks = {}\n",
    "lost_tracks = {}\n",
    "person_profiles = {}\n",
    "\n",
    "\n",
    "iou_threshold = 0.3\n",
    "motion_confidence_threshold = 0.5\n",
    "feature_update_interval = 10\n",
    "max_lost_frames = 300  # 10 seconds at 30 FPS\n",
    "face_reid_threshold = 0.75\n",
    "upper_reid_threshold = 0.65\n",
    "lower_reid_threshold = 0.6\n",
    "combined_reid_threshold = 0.7\n",
    "\n",
    "# HELPER FUNCTIONS\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"Calculate IoU between two bounding boxes\"\"\"\n",
    "    x1_1, y1_1, x2_1, y2_1 = box1\n",
    "    x1_2, y1_2, x2_2, y2_2 = box2\n",
    "    \n",
    "    x1_i = max(x1_1, x1_2)\n",
    "    y1_i = max(y1_1, y1_2)\n",
    "    x2_i = min(x2_1, x2_2)\n",
    "    y2_i = min(y2_1, y2_2)\n",
    "    \n",
    "    if x2_i <= x1_i or y2_i <= y1_i:\n",
    "        return 0.0\n",
    "    \n",
    "    intersection = (x2_i - x1_i) * (y2_i - y1_i)\n",
    "    area1 = (x2_1 - x1_1) * (y2_1 - y1_1)\n",
    "    area2 = (x2_2 - x1_2) * (y2_2 - y1_2)\n",
    "    union = area1 + area2 - intersection\n",
    "    \n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "def compute_feature_similarity(feat1, feat2):\n",
    "    \"\"\"Compute cosine similarity between features\"\"\"\n",
    "    if feat1 is None or feat2 is None:\n",
    "        return 0.0\n",
    "    try:\n",
    "        similarity = cosine_similarity(feat1.reshape(1, -1), feat2.reshape(1, -1))[0, 0]\n",
    "        return max(0.0, similarity)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def determine_pose_type(keypoints):\n",
    "    \"\"\"Determine pose type from keypoints\"\"\"\n",
    "    try:\n",
    "        kpts = keypoints.cpu().numpy() if torch.is_tensor(keypoints) else keypoints\n",
    "        if len(kpts.shape) == 3:\n",
    "            kpts = kpts[0]\n",
    "        \n",
    "        if len(kpts) < 17:\n",
    "            return \"standing\"\n",
    "        \n",
    "        # Get key points with confidence > 0.3\n",
    "        visible_points = {}\n",
    "        for name, idx in [('left_hip', 11), ('right_hip', 12), \n",
    "                        ('left_knee', 13), ('right_knee', 14),\n",
    "                        ('left_ankle', 15), ('right_ankle', 16)]:\n",
    "            if idx < len(kpts) and len(kpts[idx]) >= 3 and kpts[idx][2] > 0.3:\n",
    "                visible_points[name] = kpts[idx][:2]\n",
    "        \n",
    "        if len(visible_points) < 3:\n",
    "            return \"standing\"\n",
    "        \n",
    "        # Calculate hip-to-ankle distance (Just made so that I can get upper and lower body features separately in each situation. just wanted to try matching those separately.)\n",
    "        hip_y = []\n",
    "        ankle_y = []\n",
    "        for hip in ['left_hip', 'right_hip']:\n",
    "            if hip in visible_points:\n",
    "                hip_y.append(visible_points[hip][1])\n",
    "        for ankle in ['left_ankle', 'right_ankle']:\n",
    "            if ankle in visible_points:\n",
    "                ankle_y.append(visible_points[ankle][1])\n",
    "        \n",
    "        if hip_y and ankle_y:\n",
    "            hip_ankle_dist = abs(np.mean(ankle_y) - np.mean(hip_y))\n",
    "            \n",
    "            if hip_ankle_dist < 50:\n",
    "                return \"lying\"\n",
    "            elif hip_ankle_dist < 120:\n",
    "                return \"sitting\"\n",
    "            else:\n",
    "                return \"standing\"\n",
    "        \n",
    "        return \"standing\"\n",
    "    except:\n",
    "        return \"standing\"\n",
    "\n",
    "def create_kalman_filter(initial_bbox):\n",
    "    \"\"\"Create Kalman filter for motion tracking\"\"\"\n",
    "    kf = KalmanFilter(dim_x=8, dim_z=4)\n",
    "    \n",
    "    # State transition matrix (constant velocity model) \n",
    "    kf.F = np.array([\n",
    "        [1, 0, 0, 0, 1, 0, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 1, 0, 0],\n",
    "        [0, 0, 1, 0, 0, 0, 1, 0],\n",
    "        [0, 0, 0, 1, 0, 0, 0, 1],\n",
    "        [0, 0, 0, 0, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 1, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 1, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 1]\n",
    "    ])\n",
    "    \n",
    "    # Measurement function\n",
    "    kf.H = np.array([\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 1, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 0, 0, 0, 0]\n",
    "    ])\n",
    "    \n",
    "    # Initialize state\n",
    "    x_center = (initial_bbox[0] + initial_bbox[2]) / 2\n",
    "    y_center = (initial_bbox[1] + initial_bbox[3]) / 2\n",
    "    width = initial_bbox[2] - initial_bbox[0]\n",
    "    height = initial_bbox[3] - initial_bbox[1]\n",
    "    \n",
    "    kf.x = np.array([x_center, y_center, width, height, 0, 0, 0, 0])\n",
    "    kf.P *= 100\n",
    "    kf.R *= 10\n",
    "    kf.Q *= 0.1\n",
    "    \n",
    "    return kf\n",
    "\n",
    "def predict_motion(kalman_filter, missed_updates):\n",
    "    \"\"\"Predict next position using Kalman filter\"\"\"\n",
    "    kalman_filter.predict()\n",
    "    \n",
    "    # Extract bbox from state\n",
    "    x_center, y_center, width, height = kalman_filter.x[:4]\n",
    "    predicted_bbox = np.array([\n",
    "        x_center - width/2,\n",
    "        y_center - height/2,\n",
    "        x_center + width/2,\n",
    "        y_center + height/2\n",
    "    ])\n",
    "    \n",
    "    # Update confidence based on consecutive misses\n",
    "    prediction_confidence = max(0.1, 1.0 - (missed_updates * 0.15))\n",
    "    \n",
    "    return predicted_bbox, prediction_confidence\n",
    "\n",
    "def update_kalman_filter(kalman_filter, measurement_bbox):\n",
    "    \"\"\"Update Kalman filter with new measurement\"\"\"\n",
    "    x_center = (measurement_bbox[0] + measurement_bbox[2]) / 2\n",
    "    y_center = (measurement_bbox[1] + measurement_bbox[3]) / 2\n",
    "    width = measurement_bbox[2] - measurement_bbox[0]\n",
    "    height = measurement_bbox[3] - measurement_bbox[1]\n",
    "    \n",
    "    measurement = np.array([x_center, y_center, width, height])\n",
    "    kalman_filter.update(measurement)\n",
    "\n",
    "# FEATURE EXTRACTION FUNCTIONS\n",
    "\n",
    "\n",
    "def extract_face_region(frame, keypoints, bbox):\n",
    "    \"\"\"Extract face region using head keypoints\"\"\"\n",
    "    try:\n",
    "        kpts = keypoints.cpu().numpy() if torch.is_tensor(keypoints) else keypoints\n",
    "        if len(kpts.shape) == 3:\n",
    "            kpts = kpts[0]\n",
    "        \n",
    "        # Get head keypoints (nose, eyes, ears)\n",
    "        head_keypoints = [0, 1, 2, 3, 4]\n",
    "        head_points = []\n",
    "        for idx in head_keypoints:\n",
    "            if idx < len(kpts) and len(kpts[idx]) >= 3 and kpts[idx][2] > 0.3:\n",
    "                head_points.append(kpts[idx][:2])\n",
    "        \n",
    "        if len(head_points) >= 2:\n",
    "            head_points = np.array(head_points)\n",
    "            x_min, y_min = np.min(head_points, axis=0)\n",
    "            x_max, y_max = np.max(head_points, axis=0)\n",
    "            \n",
    "            padding = 25\n",
    "            face_x1 = max(0, int(x_min - padding))\n",
    "            face_y1 = max(0, int(y_min - padding))\n",
    "            face_x2 = min(frame.shape[1], int(x_max + padding))\n",
    "            face_y2 = min(frame.shape[0], int(y_max + padding))\n",
    "        else:\n",
    "            # Fallback to upper bbox region\n",
    "            x1, y1, x2, y2 = bbox.astype(int)\n",
    "            face_h = int((y2 - y1) * 0.35)\n",
    "            face_x1, face_y1 = x1, y1\n",
    "            face_x2, face_y2 = x2, y1 + face_h\n",
    "        \n",
    "        if face_x2 <= face_x1 or face_y2 <= face_y1:\n",
    "            return None\n",
    "        \n",
    "        face_roi = frame[face_y1:face_y2, face_x1:face_x2]\n",
    "        \n",
    "        if face_roi.shape[0] < 40 or face_roi.shape[1] < 30:\n",
    "            return None\n",
    "        \n",
    "        return face_roi, (face_x1, face_y1, face_x2, face_y2)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def extract_upper_body_region(frame, keypoints, bbox, pose_type):\n",
    "    \"\"\"Extract upper body region\"\"\"\n",
    "    try:\n",
    "        kpts = keypoints.cpu().numpy() if torch.is_tensor(keypoints) else keypoints\n",
    "        if len(kpts.shape) == 3:\n",
    "            kpts = kpts[0]\n",
    "        \n",
    "        # Get neck point from shoulders\n",
    "        neck_point = None\n",
    "        left_shoulder = kpts[5] if len(kpts) > 5 and len(kpts[5]) >= 3 and kpts[5][2] > 0.3 else None\n",
    "        right_shoulder = kpts[6] if len(kpts) > 6 and len(kpts[6]) >= 3 and kpts[6][2] > 0.3 else None\n",
    "        \n",
    "        if left_shoulder is not None and right_shoulder is not None:\n",
    "            neck_x = (left_shoulder[0] + right_shoulder[0]) / 2\n",
    "            neck_y = (left_shoulder[1] + right_shoulder[1]) / 2 - 15\n",
    "            neck_point = np.array([neck_x, neck_y])\n",
    "        \n",
    "        # Get hip points\n",
    "        hip_points = []\n",
    "        for idx in [11, 12]:  # left_hip, right_hip\n",
    "            if idx < len(kpts) and len(kpts[idx]) >= 3 and kpts[idx][2] > 0.3:\n",
    "                hip_points.append(kpts[idx][:2])\n",
    "        \n",
    "        if neck_point is not None and hip_points:\n",
    "            hip_center = np.mean(hip_points, axis=0)\n",
    "            upper_y1 = int(neck_point[1])\n",
    "            upper_y2 = int(hip_center[1])\n",
    "            \n",
    "            # Use shoulders for width\n",
    "            if left_shoulder is not None and right_shoulder is not None:\n",
    "                x_min = min(left_shoulder[0], right_shoulder[0])\n",
    "                x_max = max(left_shoulder[0], right_shoulder[0])\n",
    "                padding = 20\n",
    "                upper_x1 = max(0, int(x_min - padding))\n",
    "                upper_x2 = min(frame.shape[1], int(x_max + padding))\n",
    "            else:\n",
    "                padding = 60\n",
    "                upper_x1 = max(0, int(neck_point[0] - padding))\n",
    "                upper_x2 = min(frame.shape[1], int(neck_point[0] + padding))\n",
    "        else:\n",
    "            # Fallback to bbox-based region\n",
    "            x1, y1, x2, y2 = bbox.astype(int)\n",
    "            \n",
    "            if pose_type == \"sitting\":\n",
    "                upper_y1 = y1 + int((y2 - y1) * 0.1)\n",
    "                upper_y2 = y1 + int((y2 - y1) * 0.75)\n",
    "            elif pose_type == \"lying\":\n",
    "                upper_y1 = y1 + int((y2 - y1) * 0.2)\n",
    "                upper_y2 = y1 + int((y2 - y1) * 0.8)\n",
    "            else:  # standing\n",
    "                upper_y1 = y1 + int((y2 - y1) * 0.15)\n",
    "                upper_y2 = y1 + int((y2 - y1) * 0.65)\n",
    "            \n",
    "            upper_x1 = x1 + int((x2 - x1) * 0.1)\n",
    "            upper_x2 = x2 - int((x2 - x1) * 0.1)\n",
    "        \n",
    "        # Ensure valid region\n",
    "        upper_y1 = max(0, min(upper_y1, frame.shape[0]))\n",
    "        upper_y2 = max(upper_y1, min(upper_y2, frame.shape[0]))\n",
    "        upper_x1 = max(0, min(upper_x1, frame.shape[1]))\n",
    "        upper_x2 = max(upper_x1, min(upper_x2, frame.shape[1]))\n",
    "        \n",
    "        if upper_y2 <= upper_y1 or upper_x2 <= upper_x1:\n",
    "            return None\n",
    "        \n",
    "        upper_roi = frame[upper_y1:upper_y2, upper_x1:upper_x2]\n",
    "        \n",
    "        if upper_roi.shape[0] < 50 or upper_roi.shape[1] < 30:\n",
    "            return None\n",
    "        \n",
    "        return upper_roi, (upper_x1, upper_y1, upper_x2, upper_y2)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def extract_lower_body_region(frame, keypoints, bbox, pose_type):\n",
    "    \"\"\"Extract lower body region\"\"\"\n",
    "    if pose_type == \"lying\":\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        kpts = keypoints.cpu().numpy() if torch.is_tensor(keypoints) else keypoints\n",
    "        if len(kpts.shape) == 3:\n",
    "            kpts = kpts[0]\n",
    "        \n",
    "        # Get hip points\n",
    "        hip_points = []\n",
    "        for idx in [11, 12]:  # left_hip, right_hip\n",
    "            if idx < len(kpts) and len(kpts[idx]) >= 3 and kpts[idx][2] > 0.3:\n",
    "                hip_points.append(kpts[idx][:2])\n",
    "        \n",
    "        # Get ankle points\n",
    "        ankle_points = []\n",
    "        for idx in [15, 16]:  # left_ankle, right_ankle\n",
    "            if idx < len(kpts) and len(kpts[idx]) >= 3 and kpts[idx][2] > 0.3:\n",
    "                ankle_points.append(kpts[idx][:2])\n",
    "        \n",
    "        if hip_points and ankle_points:\n",
    "            hip_center = np.mean(hip_points, axis=0)\n",
    "            ankle_center = np.mean(ankle_points, axis=0)\n",
    "            \n",
    "            lower_y1 = int(hip_center[1])\n",
    "            lower_y2 = int(ankle_center[1]) + 20\n",
    "            \n",
    "            all_points = hip_points + ankle_points\n",
    "            all_points = np.array(all_points)\n",
    "            x_min, x_max = np.min(all_points[:, 0]), np.max(all_points[:, 0])\n",
    "            padding = 15\n",
    "            lower_x1 = max(0, int(x_min - padding))\n",
    "            lower_x2 = min(frame.shape[1], int(x_max + padding))\n",
    "        else:\n",
    "            # Fallback to bbox-based region\n",
    "            x1, y1, x2, y2 = bbox.astype(int)\n",
    "            \n",
    "            if pose_type == \"sitting\":\n",
    "                lower_y1 = y1 + int((y2 - y1) * 0.6)\n",
    "                lower_y2 = y2\n",
    "            else:  # standing\n",
    "                lower_y1 = y1 + int((y2 - y1) * 0.55)\n",
    "                lower_y2 = y2\n",
    "            \n",
    "            lower_x1 = x1 + int((x2 - x1) * 0.15)\n",
    "            lower_x2 = x2 - int((x2 - x1) * 0.15)\n",
    "        \n",
    "        # Ensure valid region\n",
    "        lower_y1 = max(0, min(lower_y1, frame.shape[0]))\n",
    "        lower_y2 = max(lower_y1, min(lower_y2, frame.shape[0]))\n",
    "        lower_x1 = max(0, min(lower_x1, frame.shape[1]))\n",
    "        lower_x2 = max(lower_x1, min(lower_x2, frame.shape[1]))\n",
    "        \n",
    "        if lower_y2 <= lower_y1 or lower_x2 <= lower_x1:\n",
    "            return None\n",
    "        \n",
    "        lower_roi = frame[lower_y1:lower_y2, lower_x1:lower_x2]\n",
    "        \n",
    "        if lower_roi.shape[0] < 40 or lower_roi.shape[1] < 25:\n",
    "            return None\n",
    "        \n",
    "        return lower_roi, (lower_x1, lower_y1, lower_x2, lower_y2)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def compute_lbp_histogram(gray_image):\n",
    "    \"\"\"Compute LBP histogram\"\"\"\n",
    "    try:\n",
    "        rows, cols = gray_image.shape\n",
    "        lbp_image = np.zeros_like(gray_image)\n",
    "        \n",
    "        for i in range(1, rows - 1):\n",
    "            for j in range(1, cols - 1):\n",
    "                center = gray_image[i, j]\n",
    "                code = 0\n",
    "                \n",
    "                neighbors = [\n",
    "                    gray_image[i-1, j-1], gray_image[i-1, j], gray_image[i-1, j+1],\n",
    "                    gray_image[i, j+1], gray_image[i+1, j+1], gray_image[i+1, j],\n",
    "                    gray_image[i+1, j-1], gray_image[i, j-1]\n",
    "                ]\n",
    "                \n",
    "                for k, neighbor in enumerate(neighbors):\n",
    "                    if neighbor > center:\n",
    "                        code |= (1 << k)\n",
    "                \n",
    "                lbp_image[i, j] = code\n",
    "        \n",
    "        hist, _ = np.histogram(lbp_image, bins=24, range=(0, 256))\n",
    "        return hist / (hist.sum() + 1e-7)\n",
    "    except:\n",
    "        return np.zeros(24)\n",
    "\n",
    "def extract_face_feature(face_roi):\n",
    "    \"\"\"Extract face feature using DeepFace\"\"\"\n",
    "    try:\n",
    "        # Validate with MTCNN\n",
    "        face_rgb = cv2.cvtColor(face_roi, cv2.COLOR_BGR2RGB)\n",
    "        boxes, probs = mtcnn.detect(face_rgb)\n",
    "        \n",
    "        if boxes is None or probs is None or len(boxes) == 0:\n",
    "            return None\n",
    "        \n",
    "        best_prob = float(np.max(probs))\n",
    "        if best_prob < 0.75:\n",
    "            return None\n",
    "        \n",
    "        # Extract DeepFace embedding\n",
    "        face_resized = cv2.resize(face_roi, (112, 112))\n",
    "        embedding_result = DeepFace.represent(\n",
    "            face_resized,\n",
    "            model_name='Facenet',\n",
    "            enforce_detection=False,\n",
    "            detector_backend='skip'\n",
    "        )\n",
    "        embedding = np.array(embedding_result[0]['embedding'])\n",
    "        \n",
    "        # Normalize\n",
    "        norm = np.linalg.norm(embedding)\n",
    "        if norm > 0:\n",
    "            embedding = embedding / norm\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "        return embedding, best_prob\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def extract_body_feature(roi):\n",
    "    \"\"\"Extract appearance feature for body region\"\"\"\n",
    "    try:\n",
    "        if roi.shape[0] < 30 or roi.shape[1] < 20:\n",
    "            return None\n",
    "        \n",
    "        features = []\n",
    "        \n",
    "        # HSV color features\n",
    "        hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "        h_hist = cv2.calcHist([hsv], [0], None, [18], [0, 180])\n",
    "        s_hist = cv2.calcHist([hsv], [1], None, [16], [0, 256])\n",
    "        v_hist = cv2.calcHist([hsv], [2], None, [16], [0, 256])\n",
    "        \n",
    "        for hist in [h_hist, s_hist, v_hist]:\n",
    "            hist_norm = cv2.normalize(hist, hist).flatten()\n",
    "            features.append(hist_norm)\n",
    "        \n",
    "        # Texture features (LBP)\n",
    "        gray_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "        gray_resized = cv2.resize(gray_roi, (48, 64))\n",
    "        lbp_hist = compute_lbp_histogram(gray_resized)\n",
    "        features.append(lbp_hist)\n",
    "        \n",
    "        # Edge features\n",
    "        edges = cv2.Canny(gray_resized, 50, 150)\n",
    "        edge_hist, _ = np.histogram(edges.sum(axis=1), bins=12)\n",
    "        edge_hist = edge_hist / (edge_hist.sum() + 1e-7)\n",
    "        features.append(edge_hist)\n",
    "        \n",
    "        # Combine all features\n",
    "        combined_feature = np.concatenate(features)\n",
    "        \n",
    "        # L2 normalize\n",
    "        norm = np.linalg.norm(combined_feature)\n",
    "        if norm > 0:\n",
    "            combined_feature = combined_feature / norm\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "        return combined_feature\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "# TRACKING FUNCTIONS\n",
    "\n",
    "def create_detection(frame, pose):\n",
    "    \"\"\"Create detection from pose result\"\"\"\n",
    "    if not hasattr(pose.pred_instances, 'bboxes') or len(pose.pred_instances.bboxes) == 0:\n",
    "        return None\n",
    "    \n",
    "    bbox = pose.pred_instances.bboxes[0]\n",
    "    if hasattr(bbox, 'cpu'):\n",
    "        bbox = bbox.cpu().numpy()\n",
    "    \n",
    "    if len(bbox) < 4 or bbox[2] - bbox[0] < 50 or bbox[3] - bbox[1] < 100:\n",
    "        return None\n",
    "    \n",
    "    keypoints = pose.pred_instances.keypoints[0]\n",
    "    confidence = float(bbox[4]) if len(bbox) > 4 else 1.0\n",
    "    pose_type = determine_pose_type(keypoints)\n",
    "    \n",
    "    # Extract features\n",
    "    face_feature = None\n",
    "    upper_feature = None\n",
    "    lower_feature = None\n",
    "    \n",
    "    if frame_count % feature_update_interval == 0:\n",
    "        # Face feature\n",
    "        face_result = extract_face_region(frame, keypoints, bbox[:4])\n",
    "        if face_result:\n",
    "            face_roi, _ = face_result\n",
    "            face_feat = extract_face_feature(face_roi)\n",
    "            if face_feat:\n",
    "                face_feature, _ = face_feat\n",
    "        \n",
    "        # Lower body feature\n",
    "        lower_result = extract_lower_body_region(frame, keypoints, bbox[:4], pose_type)\n",
    "        if lower_result:\n",
    "            lower_roi, _ = lower_result\n",
    "            lower_feature = extract_body_feature(lower_roi)\n",
    "    \n",
    "    # Always extract upper body\n",
    "    upper_result = extract_upper_body_region(frame, keypoints, bbox[:4], pose_type)\n",
    "    if upper_result:\n",
    "        upper_roi, _ = upper_result\n",
    "        upper_feature = extract_body_feature(upper_roi)\n",
    "    \n",
    "    return {\n",
    "        'bbox': bbox[:4],\n",
    "        'keypoints': keypoints,\n",
    "        'confidence': confidence,\n",
    "        'pose_type': pose_type,\n",
    "        'face_feature': face_feature,\n",
    "        'upper_feature': upper_feature,\n",
    "        'lower_feature': lower_feature\n",
    "    }\n",
    "\n",
    "def match_with_motion(detections):\n",
    "    \"\"\"Match detections with tracks using motion prediction\"\"\"\n",
    "    matches = {}\n",
    "    \n",
    "    if not active_tracks or not detections:\n",
    "        return matches\n",
    "    \n",
    "    track_ids = list(active_tracks.keys())\n",
    "    cost_matrix = np.full((len(detections), len(track_ids)), 1.0)\n",
    "    \n",
    "    for det_idx, detection in enumerate(detections):\n",
    "        for track_idx, track_id in enumerate(track_ids):\n",
    "            track = active_tracks[track_id]\n",
    "            \n",
    "            predicted_bbox, motion_confidence = predict_motion(track['kalman'], track['missed_updates'])\n",
    "            \n",
    "            if motion_confidence > motion_confidence_threshold:\n",
    "                iou = calculate_iou(detection['bbox'], predicted_bbox)\n",
    "                cost_matrix[det_idx, track_idx] = 1.0 - iou\n",
    "            else:\n",
    "                cost_matrix[det_idx, track_idx] = 0.95\n",
    "    \n",
    "    # Hungarian assignment\n",
    "    det_indices, track_indices = linear_sum_assignment(cost_matrix)\n",
    "    \n",
    "    for det_idx, track_idx in zip(det_indices, track_indices):\n",
    "        cost = cost_matrix[det_idx, track_idx]\n",
    "        if cost < (1.0 - iou_threshold):\n",
    "            track_id = track_ids[track_idx]\n",
    "            matches[det_idx] = track_id\n",
    "    \n",
    "    return matches\n",
    "\n",
    "def compute_person_similarity(detection, profile):\n",
    "    \"\"\"Compute similarity between detection and person profile\"\"\"\n",
    "    similarities = []\n",
    "    matching_components = []\n",
    "    weights = []\n",
    "    \n",
    "    # Face similarity\n",
    "    if detection['face_feature'] is not None and profile.get('face_feature') is not None:\n",
    "        face_sim = compute_feature_similarity(detection['face_feature'], profile['face_feature'])\n",
    "        if face_sim > face_reid_threshold:\n",
    "            similarities.append(face_sim)\n",
    "            matching_components.append(\"face\")\n",
    "            weights.append(0.5)\n",
    "    \n",
    "    # Upper body similarity\n",
    "    if detection['upper_feature'] is not None and profile.get('upper_feature') is not None:\n",
    "        upper_sim = compute_feature_similarity(detection['upper_feature'], profile['upper_feature'])\n",
    "        if upper_sim > upper_reid_threshold:\n",
    "            similarities.append(upper_sim)\n",
    "            matching_components.append(\"upper\")\n",
    "            weights.append(0.35)\n",
    "    \n",
    "    # Lower body similarity\n",
    "    if detection['lower_feature'] is not None and profile.get('lower_feature') is not None:\n",
    "        lower_sim = compute_feature_similarity(detection['lower_feature'], profile['lower_feature'])\n",
    "        if lower_sim > lower_reid_threshold:\n",
    "            similarities.append(lower_sim)\n",
    "            matching_components.append(\"lower\")\n",
    "            weights.append(0.15)\n",
    "    \n",
    "    if similarities and weights:\n",
    "        total_weight = sum(weights)\n",
    "        normalized_weights = [w / total_weight for w in weights]\n",
    "        combined_sim = sum(s * w for s, w in zip(similarities, normalized_weights))\n",
    "        match_description = \"+\".join(matching_components)\n",
    "        return combined_sim, match_description\n",
    "    \n",
    "    return 0.0, \"none\"\n",
    "\n",
    "def match_with_appearance(unmatched_detections):\n",
    "    \"\"\"Match with active tracks using appearance\"\"\"\n",
    "    matches = {}\n",
    "    \n",
    "    for det_idx, detection in unmatched_detections:\n",
    "        best_match_id = None\n",
    "        best_score = 0.0\n",
    "        best_match_type = \"\"\n",
    "        \n",
    "        for track_id, track in active_tracks.items():\n",
    "            if track_id in matches.values():\n",
    "                continue\n",
    "            \n",
    "            profile = person_profiles.get(track_id)\n",
    "            if not profile:\n",
    "                continue\n",
    "            \n",
    "            similarity, match_type = compute_person_similarity(detection, profile)\n",
    "            \n",
    "            if similarity > best_score and similarity > combined_reid_threshold:\n",
    "                best_score = similarity\n",
    "                best_match_id = track_id\n",
    "                best_match_type = match_type\n",
    "        \n",
    "        if best_match_id:\n",
    "            matches[det_idx] = (best_match_id, best_match_type)\n",
    "    \n",
    "    return matches\n",
    "\n",
    "def match_with_lost_tracks(unmatched_detections):\n",
    "    \"\"\"Re-identification with lost tracks\"\"\"\n",
    "    matches = {}\n",
    "    \n",
    "    for det_idx, detection in unmatched_detections:\n",
    "        best_match_id = None\n",
    "        best_score = 0.0\n",
    "        best_match_type = \"\"\n",
    "        \n",
    "        for track_id, lost_track in lost_tracks.items():\n",
    "            profile = person_profiles.get(track_id)\n",
    "            if not profile:\n",
    "                continue\n",
    "            \n",
    "            similarity, match_type = compute_person_similarity(detection, profile)\n",
    "            \n",
    "            reid_threshold = combined_reid_threshold + 0.1\n",
    "            if similarity > best_score and similarity > reid_threshold:\n",
    "                best_score = similarity\n",
    "                best_match_id = track_id\n",
    "                best_match_type = match_type\n",
    "        \n",
    "        if best_match_id:\n",
    "            matches[det_idx] = (best_match_id, best_match_type)\n",
    "    \n",
    "    return matches\n",
    "\n",
    "def update_person_profile(profile, detection):\n",
    "    \"\"\"Update person profile with new features\"\"\"\n",
    "    # Update features with exponential moving average\n",
    "    alpha = 0.3\n",
    "    \n",
    "    if detection['face_feature'] is not None:\n",
    "        if profile.get('face_feature') is None:\n",
    "            profile['face_feature'] = detection['face_feature'].copy()\n",
    "        else:\n",
    "            profile['face_feature'] = alpha * detection['face_feature'] + (1 - alpha) * profile['face_feature']\n",
    "            norm = np.linalg.norm(profile['face_feature'])\n",
    "            if norm > 0:\n",
    "                profile['face_feature'] = profile['face_feature'] / norm\n",
    "    \n",
    "    if detection['upper_feature'] is not None:\n",
    "        if profile.get('upper_feature') is None:\n",
    "            profile['upper_feature'] = detection['upper_feature'].copy()\n",
    "        else:\n",
    "            profile['upper_feature'] = alpha * detection['upper_feature'] + (1 - alpha) * profile['upper_feature']\n",
    "            norm = np.linalg.norm(profile['upper_feature'])\n",
    "            if norm > 0:\n",
    "                profile['upper_feature'] = profile['upper_feature'] / norm\n",
    "    \n",
    "    if detection['lower_feature'] is not None:\n",
    "        if profile.get('lower_feature') is None:\n",
    "            profile['lower_feature'] = detection['lower_feature'].copy()\n",
    "        else:\n",
    "            profile['lower_feature'] = alpha * detection['lower_feature'] + (1 - alpha) * profile['lower_feature']\n",
    "            norm = np.linalg.norm(profile['lower_feature'])\n",
    "            if norm > 0:\n",
    "                profile['lower_feature'] = profile['lower_feature'] / norm\n",
    "\n",
    "def create_new_track(detection):\n",
    "    \"\"\"Create new track and person profile\"\"\"\n",
    "    global next_track_id\n",
    "    \n",
    "    track = {\n",
    "        'track_id': next_track_id,\n",
    "        'kalman': create_kalman_filter(detection['bbox']),\n",
    "        'detections': deque([detection], maxlen=100),\n",
    "        'last_seen': frame_count,\n",
    "        'created_frame': frame_count,\n",
    "        'lost_frames': 0,\n",
    "        'missed_updates': 0\n",
    "    }\n",
    "    \n",
    "    profile = {\n",
    "        'person_id': next_track_id,\n",
    "        'creation_frame': frame_count,\n",
    "        'face_feature': detection['face_feature'].copy() if detection['face_feature'] is not None else None,\n",
    "        'upper_feature': detection['upper_feature'].copy() if detection['upper_feature'] is not None else None,\n",
    "        'lower_feature': detection['lower_feature'].copy() if detection['lower_feature'] is not None else None\n",
    "    }\n",
    "    \n",
    "    active_tracks[next_track_id] = track\n",
    "    person_profiles[next_track_id] = profile\n",
    "    \n",
    "    current_id = next_track_id\n",
    "    next_track_id += 1\n",
    "    \n",
    "    return current_id\n",
    "\n",
    "def update_tracking_system(detections):\n",
    "    \"\"\"Main tracking update function\"\"\"\n",
    "    global frame_count\n",
    "    frame_count += 1\n",
    "    \n",
    "    # 1. Motion-based matching\n",
    "    motion_matches = match_with_motion(detections)\n",
    "    final_matches = {}\n",
    "    \n",
    "    # Update matched tracks\n",
    "    for det_idx, track_id in motion_matches.items():\n",
    "        detection = detections[det_idx]\n",
    "        track = active_tracks[track_id]\n",
    "        \n",
    "        # Update Kalman filter\n",
    "        update_kalman_filter(track['kalman'], detection['bbox'])\n",
    "        \n",
    "        # Update track\n",
    "        track['detections'].append(detection)\n",
    "        track['last_seen'] = frame_count\n",
    "        track['lost_frames'] = 0\n",
    "        track['missed_updates'] = 0\n",
    "        \n",
    "        # Update profile\n",
    "        if frame_count % feature_update_interval == 0:\n",
    "            profile = person_profiles.get(track_id)\n",
    "            if profile:\n",
    "                update_person_profile(profile, detection)\n",
    "        \n",
    "        final_matches[det_idx] = track_id\n",
    "    \n",
    "    # 2. Appearance-based matching\n",
    "    unmatched_detections = [(i, det) for i, det in enumerate(detections) if i not in motion_matches]\n",
    "    appearance_matches = match_with_appearance(unmatched_detections)\n",
    "    \n",
    "    for det_idx, (track_id, match_type) in appearance_matches.items():\n",
    "        detection = detections[det_idx]\n",
    "        track = active_tracks[track_id]\n",
    "        \n",
    "        update_kalman_filter(track['kalman'], detection['bbox'])\n",
    "        track['detections'].append(detection)\n",
    "        track['last_seen'] = frame_count\n",
    "        track['lost_frames'] = 0\n",
    "        track['missed_updates'] = 0\n",
    "        \n",
    "        if frame_count % feature_update_interval == 0:\n",
    "            profile = person_profiles.get(track_id)\n",
    "            if profile:\n",
    "                update_person_profile(profile, detection)\n",
    "        \n",
    "        final_matches[det_idx] = track_id\n",
    "        print(f\"ID {track_id}: Appearance ({match_type})\")\n",
    "    \n",
    "    # Update unmatched list\n",
    "    unmatched_detections = [(i, det) for i, det in unmatched_detections if i not in appearance_matches]\n",
    "    \n",
    "    # 3. Re-identification with lost tracks\n",
    "    reid_matches = match_with_lost_tracks(unmatched_detections)\n",
    "    \n",
    "    for det_idx, (track_id, match_type) in reid_matches.items():\n",
    "        detection = detections[det_idx]\n",
    "        \n",
    "        # Reactivate lost track\n",
    "        reactivated_track = lost_tracks.pop(track_id)\n",
    "        reactivated_track['kalman'] = create_kalman_filter(detection['bbox'])\n",
    "        reactivated_track['detections'].append(detection)\n",
    "        reactivated_track['last_seen'] = frame_count\n",
    "        reactivated_track['lost_frames'] = 0\n",
    "        reactivated_track['missed_updates'] = 0\n",
    "        \n",
    "        active_tracks[track_id] = reactivated_track\n",
    "        final_matches[det_idx] = track_id\n",
    "        \n",
    "        profile = person_profiles.get(track_id)\n",
    "        if profile:\n",
    "            update_person_profile(profile, detection)\n",
    "        \n",
    "        print(f\"ID {track_id}: Re-identified ({match_type})\")\n",
    "    \n",
    "    # 4. Create new tracks\n",
    "    remaining_unmatched = [i for i, det in unmatched_detections if i not in reid_matches]\n",
    "    \n",
    "    for det_idx in remaining_unmatched:\n",
    "        detection = detections[det_idx]\n",
    "        new_id = create_new_track(detection)\n",
    "        final_matches[det_idx] = new_id\n",
    "        print(f\"ID {new_id}: New\")\n",
    "    \n",
    "    # 5. Handle lost tracks\n",
    "    tracks_to_remove = []\n",
    "    for track_id, track in active_tracks.items():\n",
    "        if track_id not in final_matches.values():\n",
    "            track['missed_updates'] += 1\n",
    "            track['lost_frames'] += 1\n",
    "            \n",
    "            if track['lost_frames'] > max_lost_frames:\n",
    "                if len(track['detections']) >= 10:\n",
    "                    lost_tracks[track_id] = track\n",
    "                tracks_to_remove.append(track_id)\n",
    "    \n",
    "    for track_id in tracks_to_remove:\n",
    "        if track_id in active_tracks:\n",
    "            del active_tracks[track_id]\n",
    "    \n",
    "    # Cleanup old lost tracks\n",
    "    tracks_to_cleanup = []\n",
    "    for track_id, track in lost_tracks.items():\n",
    "        if frame_count - track['last_seen'] > max_lost_frames * 2:\n",
    "            tracks_to_cleanup.append(track_id)\n",
    "    \n",
    "    for track_id in tracks_to_cleanup:\n",
    "        del lost_tracks[track_id]\n",
    "    \n",
    "    return final_matches\n",
    "\n",
    "\n",
    "# VISUALIZATION FUNCTIONS\n",
    "\n",
    "def draw_simple_tracking(frame, pose_results, person_assignments):\n",
    "    \"\"\"Simple visualization with just ID and match type\"\"\"\n",
    "    if not person_assignments:\n",
    "        return frame\n",
    "    \n",
    "    # Filter pose results\n",
    "    filtered_poses = []\n",
    "    for det_idx, track_id in person_assignments.items():\n",
    "        if det_idx < len(pose_results):\n",
    "            filtered_poses.append(pose_results[det_idx])\n",
    "    \n",
    "    if not filtered_poses:\n",
    "        return frame\n",
    "    \n",
    "    # Draw poses\n",
    "    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    data_samples = merge_data_samples(filtered_poses)\n",
    "    visualizer.add_datasample(\n",
    "        'result', img_rgb, data_sample=data_samples,\n",
    "        draw_gt=False, draw_heatmap=False, draw_bbox=False,\n",
    "        show=False, wait_time=0, out_file=None, kpt_thr=0.3\n",
    "    )\n",
    "    \n",
    "    vis_result = visualizer.get_image()\n",
    "    vis_result = cv2.cvtColor(vis_result, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Draw tracking info\n",
    "    for det_idx, track_id in person_assignments.items():\n",
    "        if det_idx < len(pose_results):\n",
    "            pose = pose_results[det_idx]\n",
    "            if hasattr(pose.pred_instances, 'bboxes') and len(pose.pred_instances.bboxes) > 0:\n",
    "                bbox = pose.pred_instances.bboxes[0]\n",
    "                if torch.is_tensor(bbox):\n",
    "                    bbox = bbox.cpu().numpy()\n",
    "                \n",
    "                x1, y1, x2, y2 = bbox[:4].astype(int)\n",
    "                \n",
    "                # Determine match type\n",
    "                track = active_tracks.get(track_id)\n",
    "                if track:\n",
    "                    if track['missed_updates'] == 0:\n",
    "                        if track['created_frame'] == frame_count:\n",
    "                            match_type = \"New\"\n",
    "                            color = (0, 255, 255)  # Yellow\n",
    "                        else:\n",
    "                            match_type = \"Motion\"\n",
    "                            color = (0, 255, 0)  # Green\n",
    "                    else:\n",
    "                        match_type = \"Appearance\"\n",
    "                        color = (255, 0, 0)  # Blue\n",
    "                else:\n",
    "                    match_type = \"Re-ID\"\n",
    "                    color = (0, 0, 255)  # Red\n",
    "                \n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(vis_result, (x1, y1), (x2, y2), color, 2)\n",
    "                \n",
    "                # Draw ID and match type\n",
    "                text = f\"ID {track_id}: {match_type}\"\n",
    "                cv2.rectangle(vis_result, (x1, y1 - 25), (x1 + len(text) * 8, y1), color, -1)\n",
    "                cv2.putText(vis_result, text, (x1 + 2, y1 - 5),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "    \n",
    "    return vis_result\n",
    "\n",
    "# MAIN \n",
    "\n",
    "def process_video(input_path, output_path):\n",
    "    \"\"\"Process video with tracking\"\"\"\n",
    "    global frame_count, next_track_id, active_tracks, lost_tracks, person_profiles\n",
    "    \n",
    "    # Reset global variables\n",
    "    frame_count = 0\n",
    "    next_track_id = 1\n",
    "    active_tracks = {}\n",
    "    lost_tracks = {}\n",
    "    person_profiles = {}\n",
    "    \n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video {input_path}\")\n",
    "        return\n",
    "    \n",
    "\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    if not writer.isOpened():\n",
    "        print(\"Error: Could not create video writer\")\n",
    "        cap.release()\n",
    "        return\n",
    "    \n",
    "    with tqdm(total=total_frames, desc=\"Processing frames\") as pbar:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                # Person detection\n",
    "                scope = detector.cfg.get('default_scope', 'mmdet')\n",
    "                if scope is not None:\n",
    "                    init_default_scope(scope)\n",
    "                \n",
    "                with torch.cuda.device(device):\n",
    "                    detect_result = inference_detector(detector, frame)\n",
    "                \n",
    "                # Process detection results\n",
    "                pred_instance = detect_result.pred_instances.cpu().numpy()\n",
    "                bboxes = np.concatenate(\n",
    "                    (pred_instance.bboxes, pred_instance.scores[:, None]), axis=1)\n",
    "                bboxes = bboxes[np.logical_and(pred_instance.labels == 0,\n",
    "                                            pred_instance.scores > 0.5)]\n",
    "                bboxes = bboxes[nms(bboxes, 0.7)][:, :4]\n",
    "                \n",
    "                # Pose estimation\n",
    "                with torch.cuda.device(device):\n",
    "                    pose_results = inference_topdown(pose_estimator, frame, bboxes)\n",
    "                \n",
    "                # Create detections\n",
    "                detections = []\n",
    "                for pose in pose_results:\n",
    "                    detection = create_detection(frame, pose)\n",
    "                    if detection:\n",
    "                        detections.append(detection)\n",
    "                \n",
    "                # Update tracking\n",
    "                person_assignments = update_tracking_system(detections)\n",
    "                \n",
    "                # Visualization\n",
    "                vis_frame = draw_simple_tracking(frame, pose_results, person_assignments)\n",
    "                writer.write(vis_frame)\n",
    "                \n",
    "                # Progress update every 50 frames\n",
    "                if frame_count % 50 == 0:\n",
    "                    torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                    print(f\"Frame {frame_count}: Active={len(active_tracks)}, Lost={len(lost_tracks)}, Total={len(person_profiles)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing frame {frame_count}: {e}\")\n",
    "                writer.write(frame)\n",
    "            \n",
    "            pbar.update(1)\n",
    "    \n",
    "\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"Processing complete. Output saved: {output_path}\")\n",
    "    print(f\"Total persons tracked: {len(person_profiles)}\")\n",
    "\n",
    "def process(input_folder, output_folder):\n",
    "\n",
    "    \n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.wmv']\n",
    "    video_files = []\n",
    "    for ext in video_extensions:\n",
    "        video_files.extend([f for f in os.listdir(input_folder) if f.lower().endswith(ext)])\n",
    "    \n",
    "    for i, video_file in enumerate(video_files):\n",
    "        print(f\"\\nProcessing video {i+1}/{len(video_files)}: {video_file}\")\n",
    "        \n",
    "        input_path = os.path.join(input_folder, video_file)\n",
    "        output_filename = os.path.splitext(video_file)[0] + '_tracked.mp4'\n",
    "        output_path = os.path.join(output_folder, output_filename)\n",
    "        process_video(input_path, output_path)    \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    print(f\"\\n outputs saved to  {output_folder}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process(\"/kaggle/input/videos\", \"/kaggle/working/output_videos/\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
