{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install ultralytics opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip3 install deep-sort-realtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install torchreid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import torchreid\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "pose_model = YOLO('yolov8n-pose.pt')\n",
    "osnet_model = torchreid.models.build_model(\n",
    "    name='osnet_x1_0',\n",
    "    num_classes=0,\n",
    "    pretrained=True\n",
    ")\n",
    "osnet_model.to(device).eval()\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((256, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "tracker = DeepSort(max_age=70, n_init=3, max_cosine_distance=0.2,\n",
    "                   nn_budget=100, embedder=None, polygon=False)\n",
    "\n",
    "\n",
    "def detect_humans_and_poses(frame, confidence_threshold=0.5):\n",
    "    results = pose_model(frame, verbose=False)\n",
    "    detections, pose_data = [], {}\n",
    "    for result in results:\n",
    "        boxes, keypoints = result.boxes, result.keypoints\n",
    "        if boxes is not None and keypoints is not None:\n",
    "            for box, kpts in zip(boxes, keypoints):\n",
    "                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "                conf, cls = box.conf[0].cpu().numpy(), box.cls[0].cpu().numpy()\n",
    "                if int(cls) == 0 and conf > confidence_threshold:\n",
    "                    detections.append([x1, y1, x2, y2, conf])\n",
    "                    keypoints_array, confidence_array = kpts.xy[0].cpu().numpy(), kpts.conf[0].cpu().numpy()\n",
    "                    pose_landmarks = [(int(x), int(y), float(c)) for (x, y), c in zip(keypoints_array, confidence_array)]\n",
    "                    pose_data[len(detections) - 1] = pose_landmarks\n",
    "    return np.array(detections), pose_data\n",
    "\n",
    "\n",
    "def extract_osnet_features(img_crops):\n",
    "    valid_crops = [transform(img).to(device) for img in img_crops if img.size != 0]\n",
    "    if not valid_crops:\n",
    "        return np.zeros((0, 512))\n",
    "    batch = torch.stack(valid_crops)\n",
    "    with torch.no_grad():\n",
    "        features = osnet_model(batch)\n",
    "        features = F.normalize(features, p=2, dim=1)\n",
    "    return features.cpu().numpy()\n",
    "\n",
    "\n",
    "def crop_person_regions(frame, detections):\n",
    "    crops, valid_detections = [], []\n",
    "    for detection in detections:\n",
    "        x1, y1, x2, y2 = detection[:4].astype(int)\n",
    "        x1, y1 = max(0, x1), max(0, y1)\n",
    "        x2, y2 = min(frame.shape[1], x2), min(frame.shape[0], y2)\n",
    "        if x2 > x1 and y2 > y1:\n",
    "            crops.append(frame[y1:y2, x1:x2])\n",
    "            valid_detections.append(detection)\n",
    "    return crops, valid_detections\n",
    "\n",
    "\n",
    "def assign_poses_to_tracks(tracks, detections, detection_poses):\n",
    "    def iou(boxA, boxB):\n",
    "        xA, yA, xB, yB = max(boxA[0], boxB[0]), max(boxA[1], boxB[1]), min(boxA[2], boxB[2]), min(boxA[3], boxB[3])\n",
    "        interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "        boxAArea, boxBArea = (boxA[2]-boxA[0])*(boxA[3]-boxA[1]), (boxB[2]-boxB[0])*(boxB[3]-boxB[1])\n",
    "        return interArea / float(boxAArea + boxBArea - interArea + 1e-6)\n",
    "\n",
    "    track_pose_data = {}\n",
    "    for track in tracks:\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "        best_iou, best_idx = 0, -1\n",
    "        for i, det in enumerate(detections):\n",
    "            val = iou(np.array(track.to_ltrb()), det[:4])\n",
    "            if val > best_iou:\n",
    "                best_iou, best_idx = val, i\n",
    "        if best_idx != -1 and best_idx in detection_poses:\n",
    "            track_pose_data[track.track_id] = detection_poses[best_idx]\n",
    "    return track_pose_data\n",
    "\n",
    "\n",
    "def draw_pose_keypoints(frame, keypoints):\n",
    "    POSE_CONNECTIONS = [\n",
    "        (0, 1), (0, 2), (1, 3), (2, 4),\n",
    "        (5, 6), (5, 7), (7, 9), (6, 8), (8, 10),\n",
    "        (5, 11), (6, 12), (11, 12),\n",
    "        (11, 13), (13, 15), (12, 14), (14, 16)\n",
    "    ]\n",
    "    for i, (x, y, conf) in enumerate(keypoints):\n",
    "        if conf > 0.3:\n",
    "            cv2.circle(frame, (x, y), 4, (255, 255, 255), -1)\n",
    "            cv2.circle(frame, (x, y), 2, (0, 0, 0), -1)\n",
    "    for c in POSE_CONNECTIONS:\n",
    "        if c[0] < len(keypoints) and c[1] < len(keypoints):\n",
    "            pt1, pt2 = keypoints[c[0]], keypoints[c[1]]\n",
    "            if pt1[2] > 0.3 and pt2[2] > 0.3:\n",
    "                cv2.line(frame, (pt1[0], pt1[1]), (pt2[0], pt2[1]), (0, 255, 0), 2)\n",
    "    return frame\n",
    "\n",
    "\n",
    "def process_frame(frame):\n",
    "    detections, detection_poses = detect_humans_and_poses(frame)\n",
    "    if len(detections) == 0:\n",
    "        return [], frame, {}\n",
    "    crops, valid_detections = crop_person_regions(frame, detections)\n",
    "    det_list = [([x1, y1, x2, y2], conf, 'person') for x1, y1, x2, y2, conf in valid_detections]\n",
    "    features = extract_osnet_features(crops)\n",
    "    tracks = tracker.update_tracks(det_list, embeds=features, frame=frame) if det_list else []\n",
    "    track_pose_data = assign_poses_to_tracks(tracks, detections, detection_poses)\n",
    "    return tracks, frame, track_pose_data\n",
    "\n",
    "\n",
    "def process_video(video_path, output_path=None, display=False):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        return\n",
    "    fps, width, height = int(cap.get(cv2.CAP_PROP_FPS)), int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    writer = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height)) if output_path else None\n",
    "    frame_count = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_count += 1\n",
    "        tracks, processed_frame, pose_data = process_frame(frame)\n",
    "        for track in tracks:\n",
    "            if not track.is_confirmed():\n",
    "                continue\n",
    "            x1, y1, x2, y2 = map(int, track.to_ltrb())\n",
    "            cv2.rectangle(processed_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(processed_frame, f'ID: {track.track_id}', (x1, y1-10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "            if track.track_id in pose_data:\n",
    "                processed_frame = draw_pose_keypoints(processed_frame, pose_data[track.track_id])\n",
    "        cv2.putText(processed_frame, f'Frame: {frame_count} | Tracks: {len(tracks)} | Poses: {len(pose_data)}',\n",
    "                    (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "        if writer:\n",
    "            writer.write(processed_frame)\n",
    "        if display:\n",
    "            cv2.imshow('Tracking', processed_frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "    cap.release()\n",
    "    if writer:\n",
    "        writer.release()\n",
    "    if display:\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_path = \"/kaggle/input/360p-video/you_video360p/SSYouTube.online_3 Years - Talks well enough for strangers to understand most of the time_360p.mp4\"\n",
    "    output_path = \"output_yolo_pose_tracked.mp4\"\n",
    "    process_video(video_path, output_path, display=False)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 13423972,
     "datasetId": 8090277,
     "sourceId": 12796183,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
