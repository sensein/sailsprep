{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2912ea",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# yolo for pose estiamtion dinov3 for appearance feature extraction for each person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b782ac06",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663e8ed3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0041f38",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"image-feature-extraction\", model=\"facebook/dinov3-vith16plus-pretrain-lvd1689m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e139d1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0581298e",
   "metadata": {},
   "source": [
    "# dinov3 only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c98d03d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "tracks = {}           # Store all tracks: {track_id: track_info}\n",
    "next_id = 1          # Next available track ID\n",
    "frame_count = 0      # Current frame number\n",
    "appearance_features = {}  # Store appearance features: {track_id: features}\n",
    "\n",
    "\n",
    "APPEARANCE_THRESHOLD = 0.4         # Minimum appearance similarity (relaxed)\n",
    "FEATURE_UPDATE_ALPHA = 0.9         # Weight for existing features (more stable)\n",
    "\n",
    "\n",
    "MAX_MISSING_FRAMES = 30    # Remove track after this many frames\n",
    "MIN_CONFIDENCE = 0.5       # Minimum detection confidence\n",
    "\n",
    "# Colors for visualization\n",
    "COLORS = [\n",
    "    (255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0),\n",
    "    (255, 0, 255), (0, 255, 255), (128, 0, 128), (255, 128, 0)\n",
    "]\n",
    "\n",
    "# COCO pose skeleton for drawing\n",
    "SKELETON = [\n",
    "    [16, 14], [14, 12], [17, 15], [15, 13], [12, 13],\n",
    "    [6, 12], [7, 13], [6, 7], [6, 8], [7, 9],\n",
    "    [8, 10], [9, 11], [2, 3], [1, 2], [1, 3],\n",
    "    [2, 4], [3, 5], [4, 6], [5, 7]\n",
    "]\n",
    "\n",
    "def load_dinov3_model():\n",
    "\n",
    "    device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "    # pipeline\n",
    "    dinov3_pipeline = pipeline(\n",
    "        \"image-feature-extraction\",\n",
    "        model=\"facebook/dinov3-vith16plus-pretrain-lvd1689m\",\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    return dinov3_pipeline\n",
    "\n",
    "def extract_appearance_features(image, detections, dinov3_pipeline):\n",
    "    \"\"\"Extract appearance features for detections\"\"\"\n",
    "    crops = []\n",
    "    h, w = image.shape[:2]\n",
    "\n",
    "    for bbox in detections:\n",
    "        x1, y1, x2, y2 = map(int, bbox[:4])\n",
    "        x1 = max(0, min(x1, w-1))\n",
    "        y1 = max(0, min(y1, h-1))\n",
    "        x2 = max(x1+1, min(x2, w))\n",
    "        y2 = max(y1+1, min(y2, h))\n",
    "\n",
    "        crop = image[y1:y2, x1:x2]\n",
    "        crop_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
    "        pil_image = Image.fromarray(crop_rgb).resize((224, 224))\n",
    "        crops.append(pil_image)\n",
    "\n",
    "    features_list = dinov3_pipeline(crops)\n",
    "\n",
    "    processed_features = []\n",
    "    for f in features_list:\n",
    "        f = np.array(f).flatten()\n",
    "        norm = np.linalg.norm(f)\n",
    "        if norm > 0:\n",
    "            f = f / norm\n",
    "        processed_features.append(f)\n",
    "\n",
    "    return processed_features\n",
    "\n",
    "def calculate_appearance_similarity(features1, features2):\n",
    "    \"\"\"Calculate cosine similarity between appearance features\"\"\"\n",
    "    dot_product = np.dot(features1, features2)\n",
    "    norm1 = np.linalg.norm(features1)\n",
    "    norm2 = np.linalg.norm(features2)\n",
    "\n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0.0\n",
    "\n",
    "    similarity = dot_product / (norm1 * norm2)\n",
    "    return max(0, similarity)\n",
    "\n",
    "def filter_detections(detections, keypoints_list):\n",
    "    \"\"\"Keep only good quality detections\"\"\"\n",
    "    if not detections:\n",
    "        return [], []\n",
    "\n",
    "    filtered_detections = []\n",
    "    filtered_keypoints = []\n",
    "\n",
    "    for i, detection in enumerate(detections):\n",
    "        x1, y1, x2, y2, conf = detection\n",
    "\n",
    "        # confidence and box size\n",
    "        width = x2 - x1\n",
    "        height = y2 - y1\n",
    "        area = width * height\n",
    "        aspect_ratio = width / height if height > 0 else 0\n",
    "\n",
    "        # Filter criteria\n",
    "        if (conf > MIN_CONFIDENCE and\n",
    "            area > 2000 and\n",
    "            0.2 < aspect_ratio < 5.0 and\n",
    "            width > 50 and height > 100):\n",
    "\n",
    "            filtered_detections.append(detection)\n",
    "            if i < len(keypoints_list):\n",
    "                filtered_keypoints.append(keypoints_list[i])\n",
    "            else:\n",
    "                filtered_keypoints.append(None)\n",
    "\n",
    "    return filtered_detections, filtered_keypoints\n",
    "\n",
    "def create_new_track(detection, keypoints, features):\n",
    "    \"\"\"Create a new track\"\"\"\n",
    "    global next_id, frame_count, tracks, appearance_features\n",
    "\n",
    "    track_id = next_id\n",
    "    next_id += 1\n",
    "\n",
    "    tracks[track_id] = {\n",
    "        'bbox': detection,\n",
    "        'keypoints': keypoints,\n",
    "        'missing_frames': 0,\n",
    "        'last_seen': frame_count,\n",
    "        'created_at': frame_count,\n",
    "        'match_type': 'new'\n",
    "    }\n",
    "\n",
    "    # appearance features storing\n",
    "    appearance_features[track_id] = features\n",
    "\n",
    "    return track_id\n",
    "\n",
    "def update_track(track_id, detection, keypoints, new_features, similarity):\n",
    "    \"\"\"Update existing track\"\"\"\n",
    "    global tracks, appearance_features, frame_count\n",
    "\n",
    "    tracks[track_id]['bbox'] = detection\n",
    "    tracks[track_id]['keypoints'] = keypoints\n",
    "    tracks[track_id]['missing_frames'] = 0\n",
    "    tracks[track_id]['last_seen'] = frame_count\n",
    "    tracks[track_id]['match_type'] = 'appearance'\n",
    "    tracks[track_id]['last_similarity'] = similarity\n",
    "\n",
    "    # Update appearance features with moving average\n",
    "    old_features = appearance_features[track_id]\n",
    "    appearance_features[track_id] = FEATURE_UPDATE_ALPHA * old_features + (1 - FEATURE_UPDATE_ALPHA) * new_features\n",
    "    # Normalize\n",
    "    norm = np.linalg.norm(appearance_features[track_id])\n",
    "    if norm > 0:\n",
    "        appearance_features[track_id] /= norm\n",
    "\n",
    "def remove_old_tracks():\n",
    "    \"\"\"Remove tracks that are too old\"\"\"\n",
    "    global tracks, appearance_features\n",
    "\n",
    "    tracks_to_remove = []\n",
    "    for track_id, track_info in tracks.items():\n",
    "        if track_info['missing_frames'] >= MAX_MISSING_FRAMES:\n",
    "            tracks_to_remove.append(track_id)\n",
    "\n",
    "    for track_id in tracks_to_remove:\n",
    "        del tracks[track_id]\n",
    "        if track_id in appearance_features:\n",
    "            del appearance_features[track_id]\n",
    "\n",
    "def appearance_matching(detections, keypoints_list, detection_features):\n",
    "    \"\"\"\n",
    "    appearance-only matching using DINOv3 features\n",
    "    Returns: list of (detection_idx, track_id, similarity) tuples\n",
    "    \"\"\"\n",
    "    global tracks, frame_count\n",
    "\n",
    "    # Get active tracks\n",
    "    active_track_ids = []\n",
    "    for track_id, track_info in tracks.items():\n",
    "        if track_info['missing_frames'] < MAX_MISSING_FRAMES:\n",
    "            active_track_ids.append(track_id)\n",
    "\n",
    "    if not active_track_ids:\n",
    "        return []  # No tracks to match\n",
    "\n",
    "    print(f\"appearance matching: {len(detections)} detections vs {len(active_track_ids)} tracks\")\n",
    "\n",
    "    matches = []\n",
    "    used_tracks = set()\n",
    "\n",
    "    # For each detection, find the best matching track based\n",
    "    for i, detection in enumerate(detections):\n",
    "        det_features = detection_features[i]\n",
    "        best_track_id = None\n",
    "        best_similarity = 0.0\n",
    "\n",
    "        # Check appearance similarity with all available tracks\n",
    "        for track_id in active_track_ids:\n",
    "            if track_id in used_tracks:\n",
    "                continue  # Track already matched\n",
    "\n",
    "            track_features = appearance_features[track_id]\n",
    "\n",
    "            # Calculate ONLY appearance similarity\n",
    "            similarity = calculate_appearance_similarity(det_features, track_features)\n",
    "\n",
    "            # Check if this is the best match so far\n",
    "            if similarity >= APPEARANCE_THRESHOLD and similarity > best_similarity:\n",
    "                best_similarity = similarity\n",
    "                best_track_id = track_id\n",
    "\n",
    "        # If we found a good match, add it\n",
    "        if best_track_id is not None:\n",
    "            matches.append((i, best_track_id, best_similarity))\n",
    "            used_tracks.add(best_track_id)\n",
    "            print(f\"  Match: detection {i} -> track {best_track_id} (similarity: {best_similarity:.3f})\")\n",
    "\n",
    "    print(f\"  Found {len(matches)}  appearance matches\")\n",
    "    return matches\n",
    "\n",
    "def update_tracks(detections, keypoints_list, image, dinov3_pipeline):\n",
    "    \"\"\"Main tracking function\"\"\"\n",
    "    global tracks, frame_count\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "    # Filter detections\n",
    "    detections, keypoints_list = filter_detections(detections, keypoints_list)\n",
    "\n",
    "    if not detections:\n",
    "        # No detections - increment missing frames for all tracks\n",
    "        for track_id in tracks:\n",
    "            tracks[track_id]['missing_frames'] += 1\n",
    "        remove_old_tracks()\n",
    "        return\n",
    "\n",
    "    # Extract appearance features for all detections\n",
    "    print(f\"Extracting DINOv3 features for {len(detections)} detections...\")\n",
    "    detection_features = extract_appearance_features(image, detections, dinov3_pipeline)\n",
    "\n",
    "    # appearance-based matching\n",
    "    matches = appearance_matching(detections, keypoints_list, detection_features)\n",
    "\n",
    "    # Update matched tracks\n",
    "    matched_detection_indices = set()\n",
    "    matched_track_ids = set()\n",
    "\n",
    "    for detection_idx, track_id, similarity in matches:\n",
    "        update_track(track_id, detections[detection_idx], keypoints_list[detection_idx],\n",
    "                    detection_features[detection_idx], similarity)\n",
    "        matched_detection_indices.add(detection_idx)\n",
    "        matched_track_ids.add(track_id)\n",
    "\n",
    "    # Create new tracks for unmatched detections\n",
    "    new_tracks_count = 0\n",
    "    for i, detection in enumerate(detections):\n",
    "        if i not in matched_detection_indices:\n",
    "            create_new_track(detection, keypoints_list[i], detection_features[i])\n",
    "            new_tracks_count += 1\n",
    "\n",
    "    # Increment missing frames for unmatched tracks\n",
    "    for track_id, track_info in tracks.items():\n",
    "        if track_id not in matched_track_ids and track_info['missing_frames'] < MAX_MISSING_FRAMES:\n",
    "            tracks[track_id]['missing_frames'] += 1\n",
    "\n",
    "    remove_old_tracks()\n",
    "\n",
    "    # matching summary\n",
    "    print(f\"  Matching summary: {len(matches)} appearance matches, {new_tracks_count} new tracks\")\n",
    "\n",
    "def draw_pose(img, keypoints, color):\n",
    "    \"\"\"Draw pose keypoints and skeleton\"\"\"\n",
    "    if keypoints is None:\n",
    "        return img\n",
    "\n",
    "    # Draw keypoints\n",
    "    for i, (x, y, conf) in enumerate(keypoints):\n",
    "        if conf > 0.3:\n",
    "            cv2.circle(img, (int(x), int(y)), 4, color, -1)\n",
    "\n",
    "    # Draw skeleton\n",
    "    for connection in SKELETON:\n",
    "        pt1_idx, pt2_idx = connection[0] - 1, connection[1] - 1\n",
    "\n",
    "        if (pt1_idx < len(keypoints) and pt2_idx < len(keypoints) and\n",
    "            keypoints[pt1_idx][2] > 0.3 and keypoints[pt2_idx][2] > 0.3):\n",
    "\n",
    "            pt1 = (int(keypoints[pt1_idx][0]), int(keypoints[pt1_idx][1]))\n",
    "            pt2 = (int(keypoints[pt2_idx][0]), int(keypoints[pt2_idx][1]))\n",
    "            cv2.line(img, pt1, pt2, color, 2)\n",
    "\n",
    "    return img\n",
    "\n",
    "def draw_tracks(img, tracks):\n",
    "    \"\"\"Draw all active tracks with similarity scores\"\"\"\n",
    "    active_tracks = {tid: info for tid, info in tracks.items()\n",
    "                    if info['missing_frames'] <= 3}\n",
    "\n",
    "    for track_id, track_info in active_tracks.items():\n",
    "\n",
    "        color = COLORS[track_id % len(COLORS)]\n",
    "\n",
    "        # Draw bbox\n",
    "        bbox = track_info['bbox']\n",
    "        x1, y1, x2, y2 = map(int, bbox[:4])\n",
    "\n",
    "        # Different thickness for new vs matched tracks\n",
    "        thickness = 2 if track_info.get('match_type') == 'appearance' else 3\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color, thickness)\n",
    "\n",
    "        # Draw track ID with similarity score if available\n",
    "        label = f'ID:{track_id}'\n",
    "        if 'last_similarity' in track_info:\n",
    "            label += f' ({track_info[\"last_similarity\"]:.2f})'\n",
    "        elif track_info.get('match_type') == 'new':\n",
    "            label += ' (NEW)'\n",
    "\n",
    "        cv2.putText(img, label, (x1, y1-10),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "        # pose\n",
    "        img = draw_pose(img, track_info['keypoints'], color)\n",
    "\n",
    "    return img\n",
    "\n",
    "def process_video(video_path, output_path):\n",
    "    \"\"\"Process a single video\"\"\"\n",
    "    print(f\"Processing: {video_path}\")\n",
    "\n",
    "    global tracks, next_id, frame_count, appearance_features\n",
    "    tracks = {}\n",
    "    next_id = 1\n",
    "    frame_count = 0\n",
    "    appearance_features = {}\n",
    "\n",
    "    yolo_model = YOLO('yolov8n-pose.pt')\n",
    "\n",
    "    dinov3_pipeline = load_dinov3_model()\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Cannot open video {video_path}\")\n",
    "        return\n",
    "\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # YOLO detection\n",
    "        results = yolo_model(frame, verbose=False)\n",
    "\n",
    "        # Extract person detections and poses\n",
    "        detections = []\n",
    "        keypoints_list = []\n",
    "\n",
    "        if results[0].boxes is not None:\n",
    "            boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "            scores = results[0].boxes.conf.cpu().numpy()\n",
    "            classes = results[0].boxes.cls.cpu().numpy()\n",
    "\n",
    "            # Filter for person class (class 0)\n",
    "            person_indices = classes == 0\n",
    "            boxes = boxes[person_indices]\n",
    "            scores = scores[person_indices]\n",
    "\n",
    "            # Get keypoints if available\n",
    "            if results[0].keypoints is not None:\n",
    "                keypoints = results[0].keypoints.xy.cpu().numpy()[person_indices]\n",
    "                keypoints_conf = results[0].keypoints.conf.cpu().numpy()[person_indices]\n",
    "\n",
    "                for i in range(len(boxes)):\n",
    "                    x1, y1, x2, y2 = boxes[i]\n",
    "                    conf = scores[i]\n",
    "                    detections.append([x1, y1, x2, y2, conf])\n",
    "\n",
    "                    # Combine keypoint coordinates with confidence\n",
    "                    kpts = []\n",
    "                    for j in range(len(keypoints[i])):\n",
    "                        x, y = keypoints[i][j]\n",
    "                        c = keypoints_conf[i][j]\n",
    "                        kpts.append([x, y, c])\n",
    "                    keypoints_list.append(kpts)\n",
    "            else:\n",
    "                # No keypoints available\n",
    "                for i in range(len(boxes)):\n",
    "                    x1, y1, x2, y2 = boxes[i]\n",
    "                    conf = scores[i]\n",
    "                    detections.append([x1, y1, x2, y2, conf])\n",
    "                    keypoints_list.append(None)\n",
    "\n",
    "        # Update tracking\n",
    "        update_tracks(detections, keypoints_list, frame, dinov3_pipeline)\n",
    "\n",
    "        # results\n",
    "        output_frame = draw_tracks(frame.copy(), tracks)\n",
    "\n",
    "        # frame info\n",
    "        active_count = len([t for t in tracks.values() if t['missing_frames'] <= 3])\n",
    "        matched_count = len([t for t in tracks.values() if t.get('match_type') == 'appearance'])\n",
    "        new_count = len([t for t in tracks.values() if t.get('match_type') == 'new'])\n",
    "\n",
    "        info_text = f\"Frame: {frame_count}/{total_frames} | Active: {active_count} | Matched: {matched_count} | New: {new_count}\"\n",
    "        cv2.putText(output_frame, info_text, (10, 30),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "        writer.write(output_frame)\n",
    "\n",
    "        if frame_count % 50 == 0:\n",
    "            print(f\"Processed {frame_count}/{total_frames} frames\")\n",
    "\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "\n",
    "    print(f\"Video saved to: {output_path}\")\n",
    "    print(f\"Total unique people tracked: {next_id - 1}\")\n",
    "\n",
    "    tracking_data = {\n",
    "        'video_file': os.path.basename(video_path),\n",
    "        'tracks': tracks,\n",
    "        'appearance_features': appearance_features,\n",
    "        'total_frames': frame_count,\n",
    "        'fps': fps,\n",
    "        'parameters': {\n",
    "            'appearance_threshold': APPEARANCE_THRESHOLD,\n",
    "            'feature_update_alpha': FEATURE_UPDATE_ALPHA\n",
    "        }\n",
    "    }\n",
    "\n",
    "    data_file = output_path.replace('.mp4', '__appearance_tracking_data.pkl')\n",
    "    with open(data_file, 'wb') as f:\n",
    "        pickle.dump(tracking_data, f)\n",
    "    print(f\"Tracking data saved to: {data_file}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to process all videos\"\"\"\n",
    "\n",
    "    input_folder = '/kaggle/input/360p-video/you_video360p'\n",
    "    output_folder = '/kaggle/working/_appearance_tracking_output'\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    video_extensions = ['.mp4', '.avi', '.mov', '.mkv']\n",
    "    video_files = []\n",
    "\n",
    "    if os.path.exists(input_folder):\n",
    "        for file in os.listdir(input_folder):\n",
    "            if any(file.lower().endswith(ext) for ext in video_extensions):\n",
    "                video_files.append(file)\n",
    "    else:\n",
    "        print(f\"Input folder not found: {input_folder}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(video_files)} video(s) to process\")\n",
    "    print(f\"Using  appearance matching ONLY (threshold ≥{APPEARANCE_THRESHOLD})\")\n",
    "\n",
    "    for video_file in video_files:\n",
    "        print(f\"\\n=== Processing: {video_file} ===\")\n",
    "\n",
    "        video_path = os.path.join(input_folder, video_file)\n",
    "        output_path = os.path.join(output_folder,\n",
    "                                  video_file.replace('.mp4', 'appearance_tracked.mp4'))\n",
    "\n",
    "        try:\n",
    "            process_video(video_path, output_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {video_file}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "\n",
    "    print(\"\\nAll videos processed \")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469181d2",
   "metadata": {},
   "source": [
    "# dinov3 and distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8660b6fb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "tracks = {}           # Store all tracks: {track_id: track_info}\n",
    "next_id = 1          # Next available track ID\n",
    "frame_count = 0      # Current frame number\n",
    "appearance_features = {}  # Store appearance features: {track_id: features}\n",
    "\n",
    "APPEARANCE_THRESHOLD = 0.6         # Minimum appearance similarity\n",
    "MAX_DISTANCE_FOR_APPEARANCE = 200  # Max distance to consider for matching\n",
    "FEATURE_UPDATE_ALPHA = 0.8         # Weight for existing features in moving average\n",
    "\n",
    "\n",
    "MAX_MISSING_FRAMES = 30    # Remove track after this many frames\n",
    "MIN_CONFIDENCE = 0.5       # Minimum detection confidence\n",
    "\n",
    "\n",
    "COLORS = [\n",
    "    (255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0),\n",
    "    (255, 0, 255), (0, 255, 255), (128, 0, 128), (255, 128, 0)\n",
    "]\n",
    "\n",
    "SKELETON = [\n",
    "    [16, 14], [14, 12], [17, 15], [15, 13], [12, 13],\n",
    "    [6, 12], [7, 13], [6, 7], [6, 8], [7, 9],\n",
    "    [8, 10], [9, 11], [2, 3], [1, 2], [1, 3],\n",
    "    [2, 4], [3, 5], [4, 6], [5, 7]\n",
    "]\n",
    "\n",
    "def load_dinov3_model():\n",
    "\n",
    "    device = 0 if torch.cuda.is_available() else -1\n",
    "    dinov3_pipeline = pipeline(\n",
    "        \"image-feature-extraction\",\n",
    "        model=\"facebook/dinov3-vith16plus-pretrain-lvd1689m\",\n",
    "        device=device\n",
    "    )\n",
    "    return dinov3_pipeline\n",
    "\n",
    "def extract_appearance_features(image, detections, dinov3_pipeline):\n",
    "    \"\"\"Extract appearance features for detections\"\"\"\n",
    "    crops = []\n",
    "    h, w = image.shape[:2]\n",
    "\n",
    "    for bbox in detections:\n",
    "        x1, y1, x2, y2 = map(int, bbox[:4])\n",
    "        x1 = max(0, min(x1, w-1))\n",
    "        y1 = max(0, min(y1, h-1))\n",
    "        x2 = max(x1+1, min(x2, w))\n",
    "        y2 = max(y1+1, min(y2, h))\n",
    "\n",
    "        crop = image[y1:y2, x1:x2]\n",
    "        crop_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
    "        pil_image = Image.fromarray(crop_rgb).resize((224, 224))\n",
    "        crops.append(pil_image)\n",
    "\n",
    "\n",
    "    features_list = dinov3_pipeline(crops)\n",
    "\n",
    "    # Normalize each feature vector\n",
    "    processed_features = []\n",
    "    for f in features_list:\n",
    "        f = np.array(f).flatten()\n",
    "        norm = np.linalg.norm(f)\n",
    "        if norm > 0:\n",
    "            f = f / norm\n",
    "        processed_features.append(f)\n",
    "\n",
    "    return processed_features\n",
    "\n",
    "def calculate_distance(bbox1, bbox2):\n",
    "    \"\"\"Calculate distance between centers of two bounding boxes\"\"\"\n",
    "    center1 = [(bbox1[0] + bbox1[2])/2, (bbox1[1] + bbox1[3])/2]\n",
    "    center2 = [(bbox2[0] + bbox2[2])/2, (bbox2[1] + bbox2[3])/2]\n",
    "    return np.sqrt((center1[0] - center2[0])**2 + (center1[1] - center2[1])**2)\n",
    "\n",
    "def calculate_appearance_similarity(features1, features2):\n",
    "    \"\"\"Calculate cosine similarity between appearance features\"\"\"\n",
    "    dot_product = np.dot(features1, features2)\n",
    "    norm1 = np.linalg.norm(features1)\n",
    "    norm2 = np.linalg.norm(features2)\n",
    "\n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0.0\n",
    "\n",
    "    similarity = dot_product / (norm1 * norm2)\n",
    "    return max(0, similarity)  # Clip negative values\n",
    "\n",
    "def filter_detections(detections, keypoints_list):\n",
    "    \"\"\"Keep only good quality detections\"\"\"\n",
    "    if not detections:\n",
    "        return [], []\n",
    "\n",
    "    filtered_detections = []\n",
    "    filtered_keypoints = []\n",
    "\n",
    "    for i, detection in enumerate(detections):\n",
    "        x1, y1, x2, y2, conf = detection\n",
    "\n",
    "        # confidence and box size\n",
    "        width = x2 - x1\n",
    "        height = y2 - y1\n",
    "        area = width * height\n",
    "        aspect_ratio = width / height if height > 0 else 0\n",
    "\n",
    "        # Filter criteria\n",
    "        if (conf > MIN_CONFIDENCE and\n",
    "            area > 2000 and\n",
    "            0.2 < aspect_ratio < 5.0 and\n",
    "            width > 50 and height > 100):\n",
    "\n",
    "            filtered_detections.append(detection)\n",
    "            if i < len(keypoints_list):\n",
    "                filtered_keypoints.append(keypoints_list[i])\n",
    "            else:\n",
    "                filtered_keypoints.append(None)\n",
    "\n",
    "    return filtered_detections, filtered_keypoints\n",
    "\n",
    "def create_new_track(detection, keypoints, features):\n",
    "    \"\"\"Create a new track\"\"\"\n",
    "    global next_id, frame_count, tracks, appearance_features\n",
    "\n",
    "    track_id = next_id\n",
    "    next_id += 1\n",
    "\n",
    "    tracks[track_id] = {\n",
    "        'bbox': detection,\n",
    "        'keypoints': keypoints,\n",
    "        'missing_frames': 0,\n",
    "        'last_seen': frame_count,\n",
    "        'created_at': frame_count,\n",
    "        'match_type': 'new'\n",
    "    }\n",
    "\n",
    "    # Store appearance features\n",
    "    appearance_features[track_id] = features\n",
    "\n",
    "    return track_id\n",
    "\n",
    "def update_track(track_id, detection, keypoints, new_features, similarity):\n",
    "    \"\"\"Update existing track\"\"\"\n",
    "    global tracks, appearance_features, frame_count\n",
    "\n",
    "    tracks[track_id]['bbox'] = detection\n",
    "    tracks[track_id]['keypoints'] = keypoints\n",
    "    tracks[track_id]['missing_frames'] = 0\n",
    "    tracks[track_id]['last_seen'] = frame_count\n",
    "    tracks[track_id]['match_type'] = 'appearance'\n",
    "    tracks[track_id]['last_similarity'] = similarity\n",
    "\n",
    "    # Update appearance features with moving average\n",
    "    old_features = appearance_features[track_id]\n",
    "    appearance_features[track_id] = FEATURE_UPDATE_ALPHA * old_features + (1 - FEATURE_UPDATE_ALPHA) * new_features\n",
    "    # Normalize\n",
    "    norm = np.linalg.norm(appearance_features[track_id])\n",
    "    if norm > 0:\n",
    "        appearance_features[track_id] /= norm\n",
    "\n",
    "def remove_old_tracks():\n",
    "    \"\"\"Remove tracks that are too old\"\"\"\n",
    "    global tracks, appearance_features\n",
    "\n",
    "    tracks_to_remove = []\n",
    "    for track_id, track_info in tracks.items():\n",
    "        if track_info['missing_frames'] >= MAX_MISSING_FRAMES:\n",
    "            tracks_to_remove.append(track_id)\n",
    "\n",
    "    for track_id in tracks_to_remove:\n",
    "        del tracks[track_id]\n",
    "        if track_id in appearance_features:\n",
    "            del appearance_features[track_id]\n",
    "\n",
    "def similarity_matching(detections, keypoints_list, detection_features, image):\n",
    "    \"\"\"\n",
    "    Simple highest similarity matching using DINOv3 features\n",
    "    Returns: list of (detection_idx, track_id, similarity) tuples\n",
    "    \"\"\"\n",
    "    global tracks, frame_count\n",
    "\n",
    "    # active tracks\n",
    "    active_track_ids = []\n",
    "    for track_id, track_info in tracks.items():\n",
    "        if track_info['missing_frames'] < MAX_MISSING_FRAMES:\n",
    "            active_track_ids.append(track_id)\n",
    "\n",
    "    if not active_track_ids:\n",
    "        return []  # No tracks to match\n",
    "\n",
    "    print(f\"  Similarity matching: {len(detections)} detections vs {len(active_track_ids)} tracks\")\n",
    "\n",
    "    matches = []\n",
    "    used_tracks = set()\n",
    "\n",
    "    # For each detection, finding the best matching track\n",
    "    for i, detection in enumerate(detections):\n",
    "        det_features = detection_features[i]\n",
    "        best_track_id = None\n",
    "        best_similarity = 0.0\n",
    "\n",
    "        # similarity with all available tracks\n",
    "        for track_id in active_track_ids:\n",
    "            if track_id in used_tracks:\n",
    "                continue  # Track already matched\n",
    "\n",
    "            track_bbox = tracks[track_id]['bbox']\n",
    "            track_features = appearance_features[track_id]\n",
    "\n",
    "            # distance between detection and track\n",
    "            distance = calculate_distance(detection[:4], track_bbox[:4])\n",
    "\n",
    "            # consider if distance is reasonable\n",
    "            if distance <= MAX_DISTANCE_FOR_APPEARANCE:\n",
    "                # Calculate appearance similarity\n",
    "                similarity = calculate_appearance_similarity(det_features, track_features)\n",
    "\n",
    "                # Check if this is the best match so far\n",
    "                if similarity >= APPEARANCE_THRESHOLD and similarity > best_similarity:\n",
    "                    best_similarity = similarity\n",
    "                    best_track_id = track_id\n",
    "\n",
    "        # If we found a good match, add it\n",
    "        if best_track_id is not None:\n",
    "            matches.append((i, best_track_id, best_similarity))\n",
    "            used_tracks.add(best_track_id)\n",
    "\n",
    "    return matches\n",
    "\n",
    "def update_tracks(detections, keypoints_list, image, dinov3_pipeline):\n",
    "    \"\"\" tracking function using only appearance features\"\"\"\n",
    "    global tracks, frame_count\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "    # Filter detections\n",
    "    detections, keypoints_list = filter_detections(detections, keypoints_list)\n",
    "\n",
    "    if not detections:\n",
    "        # No detections - increment missing frames for all tracks\n",
    "        for track_id in tracks:\n",
    "            tracks[track_id]['missing_frames'] += 1\n",
    "        remove_old_tracks()\n",
    "        return\n",
    "\n",
    "    # Extract appearance features for all detections\n",
    "    detection_features = extract_appearance_features(image, detections, dinov3_pipeline)\n",
    "\n",
    "    # similarity-based matching\n",
    "    matches = similarity_matching(detections, keypoints_list, detection_features, image)\n",
    "\n",
    "    # Update matched tracks\n",
    "    matched_detection_indices = set()\n",
    "    matched_track_ids = set()\n",
    "\n",
    "    for detection_idx, track_id, similarity in matches:\n",
    "        update_track(track_id, detections[detection_idx], keypoints_list[detection_idx],\n",
    "                    detection_features[detection_idx], similarity)\n",
    "        matched_detection_indices.add(detection_idx)\n",
    "        matched_track_ids.add(track_id)\n",
    "\n",
    "    # Create new tracks for unmatched detections\n",
    "    new_tracks_count = 0\n",
    "    for i, detection in enumerate(detections):\n",
    "        if i not in matched_detection_indices:\n",
    "            create_new_track(detection, keypoints_list[i], detection_features[i])\n",
    "            new_tracks_count += 1\n",
    "\n",
    "    # Increment missing frames for unmatched tracks\n",
    "    for track_id, track_info in tracks.items():\n",
    "        if track_id not in matched_track_ids and track_info['missing_frames'] < MAX_MISSING_FRAMES:\n",
    "            tracks[track_id]['missing_frames'] += 1\n",
    "\n",
    "    remove_old_tracks()\n",
    "\n",
    "\n",
    "def draw_pose(img, keypoints, color):\n",
    "    \"\"\"Draw pose keypoints and skeleton\"\"\"\n",
    "    if keypoints is None:\n",
    "        return img\n",
    "\n",
    "    # Draw keypoints\n",
    "    for i, (x, y, conf) in enumerate(keypoints):\n",
    "        if conf > 0.3:\n",
    "            cv2.circle(img, (int(x), int(y)), 4, color, -1)\n",
    "\n",
    "    # Draw skeleton\n",
    "    for connection in SKELETON:\n",
    "        pt1_idx, pt2_idx = connection[0] - 1, connection[1] - 1\n",
    "\n",
    "        if (pt1_idx < len(keypoints) and pt2_idx < len(keypoints) and\n",
    "            keypoints[pt1_idx][2] > 0.3 and keypoints[pt2_idx][2] > 0.3):\n",
    "\n",
    "            pt1 = (int(keypoints[pt1_idx][0]), int(keypoints[pt1_idx][1]))\n",
    "            pt2 = (int(keypoints[pt2_idx][0]), int(keypoints[pt2_idx][1]))\n",
    "            cv2.line(img, pt1, pt2, color, 2)\n",
    "\n",
    "    return img\n",
    "\n",
    "def draw_tracks(img, tracks):\n",
    "    \"\"\"Draw all active tracks with similarity scores\"\"\"\n",
    "    active_tracks = {tid: info for tid, info in tracks.items()\n",
    "                    if info['missing_frames'] <= 3}\n",
    "\n",
    "    for track_id, track_info in active_tracks.items():\n",
    "        color = COLORS[track_id % len(COLORS)]\n",
    "\n",
    "        # Draw bbox\n",
    "        bbox = track_info['bbox']\n",
    "        x1, y1, x2, y2 = map(int, bbox[:4])\n",
    "\n",
    "        # Different thickness for new vs matched tracks\n",
    "        thickness = 2 if track_info.get('match_type') == 'appearance' else 3\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color, thickness)\n",
    "\n",
    "        # Draw track ID with similarity score if available\n",
    "        label = f'ID:{track_id}'\n",
    "        if 'last_similarity' in track_info:\n",
    "            label += f' ({track_info[\"last_similarity\"]:.2f})'\n",
    "        elif track_info.get('match_type') == 'new':\n",
    "            label += ' (NEW)'\n",
    "\n",
    "        cv2.putText(img, label, (x1, y1-10),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "        #pose\n",
    "        img = draw_pose(img, track_info['keypoints'], color)\n",
    "\n",
    "    return img\n",
    "\n",
    "def process_video(video_path, output_path):\n",
    "    \"\"\"Process a single video\"\"\"\n",
    "    print(f\"Processing: {video_path}\")\n",
    "\n",
    "    global tracks, next_id, frame_count, appearance_features\n",
    "    tracks = {}\n",
    "    next_id = 1\n",
    "    frame_count = 0\n",
    "    appearance_features = {}\n",
    "\n",
    "    yolo_model = YOLO('yolov8n-pose.pt')\n",
    "\n",
    "    dinov3_pipeline = load_dinov3_model()\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Cannot open video {video_path}\")\n",
    "        return\n",
    "\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        results = yolo_model(frame, verbose=False)\n",
    "\n",
    "        detections = []\n",
    "        keypoints_list = []\n",
    "\n",
    "        if results[0].boxes is not None:\n",
    "            boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "            scores = results[0].boxes.conf.cpu().numpy()\n",
    "            classes = results[0].boxes.cls.cpu().numpy()\n",
    "\n",
    "            person_indices = classes == 0\n",
    "            boxes = boxes[person_indices]\n",
    "            scores = scores[person_indices]\n",
    "\n",
    "\n",
    "            if results[0].keypoints is not None:\n",
    "                keypoints = results[0].keypoints.xy.cpu().numpy()[person_indices]\n",
    "                keypoints_conf = results[0].keypoints.conf.cpu().numpy()[person_indices]\n",
    "\n",
    "                for i in range(len(boxes)):\n",
    "                    x1, y1, x2, y2 = boxes[i]\n",
    "                    conf = scores[i]\n",
    "                    detections.append([x1, y1, x2, y2, conf])\n",
    "\n",
    "\n",
    "                    kpts = []\n",
    "                    for j in range(len(keypoints[i])):\n",
    "                        x, y = keypoints[i][j]\n",
    "                        c = keypoints_conf[i][j]\n",
    "                        kpts.append([x, y, c])\n",
    "                    keypoints_list.append(kpts)\n",
    "            else:\n",
    "\n",
    "                for i in range(len(boxes)):\n",
    "                    x1, y1, x2, y2 = boxes[i]\n",
    "                    conf = scores[i]\n",
    "                    detections.append([x1, y1, x2, y2, conf])\n",
    "                    keypoints_list.append(None)\n",
    "\n",
    "        # Update tracking using  appearance features\n",
    "        update_tracks(detections, keypoints_list, frame, dinov3_pipeline)\n",
    "\n",
    "        # results\n",
    "        output_frame = draw_tracks(frame.copy(), tracks)\n",
    "\n",
    "        # frame info\n",
    "        active_count = len([t for t in tracks.values() if t['missing_frames'] <= 3])\n",
    "        matched_count = len([t for t in tracks.values() if t.get('match_type') == 'appearance'])\n",
    "        new_count = len([t for t in tracks.values() if t.get('match_type') == 'new'])\n",
    "\n",
    "        info_text = f\"Frame: {frame_count}/{total_frames} | Active: {active_count} | Matched: {matched_count} | New: {new_count}\"\n",
    "        cv2.putText(output_frame, info_text, (10, 30),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "        writer.write(output_frame)\n",
    "\n",
    "        # Progress update\n",
    "        if frame_count % 50 == 0:\n",
    "            print(f\"Processed {frame_count}/{total_frames} frames\")\n",
    "\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "\n",
    "\n",
    "    tracking_data = {\n",
    "        'video_file': os.path.basename(video_path),\n",
    "        'tracks': tracks,\n",
    "        'appearance_features': appearance_features,\n",
    "        'total_frames': frame_count,\n",
    "        'fps': fps,\n",
    "        'parameters': {\n",
    "            'appearance_threshold': APPEARANCE_THRESHOLD,\n",
    "            'max_distance_for_appearance': MAX_DISTANCE_FOR_APPEARANCE,\n",
    "            'feature_update_alpha': FEATURE_UPDATE_ALPHA\n",
    "        }\n",
    "    }\n",
    "\n",
    "    data_file = output_path.replace('.mp4', '_dinov3_tracking_data.pkl')\n",
    "    with open(data_file, 'wb') as f:\n",
    "        pickle.dump(tracking_data, f)\n",
    "    print(f\"Tracking data saved to: {data_file}\")\n",
    "\n",
    "def main():\n",
    "\n",
    "    input_folder = '/kaggle/input/360p-video/you_video360p'\n",
    "    output_folder = '/kaggle/working/dinov3_tracking_output'\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    video_extensions = ['.mp4', '.avi', '.mov', '.mkv']\n",
    "    video_files = []\n",
    "\n",
    "    if os.path.exists(input_folder):\n",
    "        for file in os.listdir(input_folder):\n",
    "            if any(file.lower().endswith(ext) for ext in video_extensions):\n",
    "                video_files.append(file)\n",
    "    else:\n",
    "        print(f\"Input folder not found: {input_folder}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(video_files)} video(s) to process\")\n",
    "    print(f\"Using simple highest similarity matching (threshold ≥{APPEARANCE_THRESHOLD})\")\n",
    "\n",
    "    for video_file in video_files:\n",
    "\n",
    "        video_path = os.path.join(input_folder, video_file)\n",
    "        output_path = os.path.join(output_folder,\n",
    "                                  video_file.replace('.mp4', '_dinov3_tracked.mp4'))\n",
    "\n",
    "        process_video(video_path, output_path)\n",
    "    print(\"\\n All videos processed \")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60d11c0",
   "metadata": {},
   "source": [
    "# iou fall back dinov3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edcd8f9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "\n",
    "tracks = {}           # Store all tracks: {track_id: track_info}\n",
    "next_id = 1          # Next available track ID\n",
    "frame_count = 0      # Current frame number\n",
    "appearance_features = {}  # Store appearance features: {track_id: features}\n",
    "\n",
    "\n",
    "IOU_MATCH_THRESHOLD = 0.6      # Minimum IoU for direct matching\n",
    "MIN_IOU_FOR_CONSIDERATION = 0.1 # Minimum IoU to even consider a match\n",
    "\n",
    "\n",
    "MAX_DISTANCE_FOR_APPEARANCE = 150  # Max distance to use appearance\n",
    "APPEARANCE_THRESHOLD = 0.6         # Minimum appearance similarity\n",
    "APPEARANCE_MATCH_FRAMES = 10       # Use appearance for tracks missing < N frames\n",
    "\n",
    "\n",
    "MAX_MISSING_FRAMES = 30    # Remove track after this many frames\n",
    "MIN_CONFIDENCE = 0.5       # Minimum detection confidence\n",
    "\n",
    "\n",
    "COLORS = [\n",
    "    (255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0),\n",
    "    (255, 0, 255), (0, 255, 255), (128, 0, 128), (255, 128, 0)\n",
    "]\n",
    "\n",
    "\n",
    "SKELETON = [\n",
    "    [16, 14], [14, 12], [17, 15], [15, 13], [12, 13],\n",
    "    [6, 12], [7, 13], [6, 7], [6, 8], [7, 9],\n",
    "    [8, 10], [9, 11], [2, 3], [1, 2], [1, 3],\n",
    "    [2, 4], [3, 5], [4, 6], [5, 7]\n",
    "]\n",
    "\n",
    "def load_dinov3_model():\n",
    "    device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "    # pipeline\n",
    "    dinov3_pipeline = pipeline(\n",
    "        \"image-feature-extraction\",\n",
    "        model=\"facebook/dinov3-vith16plus-pretrain-lvd1689m\",\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    return dinov3_pipeline\n",
    "\n",
    "\n",
    "'''def extract_appearance_features(image, bbox, dinov3_pipeline):\n",
    "    \"\"\"Extract appearance features using transformers pipeline\"\"\"\n",
    "    x1, y1, x2, y2 = map(int, bbox[:4])\n",
    "\n",
    "    # Make sure bbox is valid\n",
    "    h, w = image.shape[:2]\n",
    "    x1 = max(0, min(x1, w-1))\n",
    "    y1 = max(0, min(y1, h-1))\n",
    "    x2 = max(x1+1, min(x2, w))\n",
    "    y2 = max(y1+1, min(y2, h))\n",
    "\n",
    "    # Extract person crop\n",
    "    person_crop = image[y1:y2, x1:x2]\n",
    "\n",
    "    # Convert BGR to RGB\n",
    "    person_crop_rgb = cv2.cvtColor(person_crop, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Convert to PIL Image\n",
    "    pil_image = Image.fromarray(person_crop_rgb)\n",
    "\n",
    "    # Resize to standard size for better features\n",
    "    pil_image = pil_image.resize((224, 224))\n",
    "\n",
    "    # Extract features using the transformers pipeline\n",
    "    features = dinov3_pipeline([pil_image])[0]\n",
    "\n",
    "    # Convert to numpy array and flatten\n",
    "    features = np.array(features).flatten()\n",
    "\n",
    "    # Normalize the features\n",
    "    norm = np.linalg.norm(features)\n",
    "    if norm > 0:\n",
    "        features = features / norm\n",
    "\n",
    "    return features'''\n",
    "\n",
    "\n",
    "\n",
    "def extract_appearance_features(image, detections, dinov3_pipeline):\n",
    "    \"\"\"Extract appearance features for detections\"\"\"\n",
    "    crops = []\n",
    "    h, w = image.shape[:2]\n",
    "\n",
    "    for bbox in detections:\n",
    "        x1, y1, x2, y2 = map(int, bbox[:4])\n",
    "        x1 = max(0, min(x1, w-1))\n",
    "        y1 = max(0, min(y1, h-1))\n",
    "        x2 = max(x1+1, min(x2, w))\n",
    "        y2 = max(y1+1, min(y2, h))\n",
    "\n",
    "        crop = image[y1:y2, x1:x2]\n",
    "        crop_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
    "        pil_image = Image.fromarray(crop_rgb).resize((224, 224))\n",
    "        crops.append(pil_image)\n",
    "\n",
    "    features_list = dinov3_pipeline(crops)\n",
    "\n",
    "    processed_features = []\n",
    "    for f in features_list:\n",
    "        f = np.array(f).flatten()\n",
    "        norm = np.linalg.norm(f)\n",
    "        if norm > 0:\n",
    "            f = f / norm\n",
    "        processed_features.append(f)\n",
    "\n",
    "    return processed_features\n",
    "\n",
    "\n",
    "def calculate_distance(bbox1, bbox2):\n",
    "    \"\"\"Calculate distance between centers of two bounding boxes\"\"\"\n",
    "    center1 = [(bbox1[0] + bbox1[2])/2, (bbox1[1] + bbox1[3])/2]\n",
    "    center2 = [(bbox2[0] + bbox2[2])/2, (bbox2[1] + bbox2[3])/2]\n",
    "    return np.sqrt((center1[0] - center2[0])**2 + (center1[1] - center2[1])**2)\n",
    "\n",
    "def calculate_iou(bbox1, bbox2):\n",
    "    \"\"\"Calculate Intersection over Union\"\"\"\n",
    "    x1 = max(bbox1[0], bbox2[0])\n",
    "    y1 = max(bbox1[1], bbox2[1])\n",
    "    x2 = min(bbox1[2], bbox2[2])\n",
    "    y2 = min(bbox1[3], bbox2[3])\n",
    "\n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return 0.0\n",
    "\n",
    "    intersection = (x2 - x1) * (y2 - y1)\n",
    "    area1 = (bbox1[2] - bbox1[0]) * (bbox1[3] - bbox1[1])\n",
    "    area2 = (bbox2[2] - bbox2[0]) * (bbox2[3] - bbox2[1])\n",
    "    union = area1 + area2 - intersection\n",
    "\n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "def calculate_appearance_similarity(features1, features2):\n",
    "    \"\"\"Calculate cosine similarity between appearance features\"\"\"\n",
    "    dot_product = np.dot(features1, features2)\n",
    "    norm1 = np.linalg.norm(features1)\n",
    "    norm2 = np.linalg.norm(features2)\n",
    "\n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0.0\n",
    "\n",
    "    similarity = dot_product / (norm1 * norm2)\n",
    "    return max(0, similarity)  # Clip negative values\n",
    "\n",
    "def filter_detections(detections, keypoints_list):\n",
    "    \"\"\"Keep only good quality detections\"\"\"\n",
    "    if not detections:\n",
    "        return [], []\n",
    "\n",
    "    filtered_detections = []\n",
    "    filtered_keypoints = []\n",
    "\n",
    "    for i, detection in enumerate(detections):\n",
    "        x1, y1, x2, y2, conf = detection\n",
    "\n",
    "        # Check confidence and box size\n",
    "        width = x2 - x1\n",
    "        height = y2 - y1\n",
    "        area = width * height\n",
    "        aspect_ratio = width / height if height > 0 else 0\n",
    "\n",
    "        # Filter criteria\n",
    "        if (conf > MIN_CONFIDENCE and\n",
    "            area > 2000 and\n",
    "            0.2 < aspect_ratio < 5.0 and\n",
    "            width > 50 and height > 100):\n",
    "\n",
    "            filtered_detections.append(detection)\n",
    "            if i < len(keypoints_list):\n",
    "                filtered_keypoints.append(keypoints_list[i])\n",
    "            else:\n",
    "                filtered_keypoints.append(None)\n",
    "\n",
    "    return filtered_detections, filtered_keypoints\n",
    "\n",
    "def create_new_track(detection, keypoints, features):\n",
    "    \"\"\"Create a new track\"\"\"\n",
    "    global next_id, frame_count, tracks, appearance_features\n",
    "\n",
    "    track_id = next_id\n",
    "    next_id += 1\n",
    "\n",
    "    tracks[track_id] = {\n",
    "        'bbox': detection,\n",
    "        'keypoints': keypoints,\n",
    "        'missing_frames': 0,\n",
    "        'last_seen': frame_count,\n",
    "        'created_at': frame_count,\n",
    "        'match_type': 'new'\n",
    "    }\n",
    "\n",
    "    # Store appearance features\n",
    "    appearance_features[track_id] = features\n",
    "\n",
    "    return track_id\n",
    "\n",
    "def update_track(track_id, detection, keypoints, new_features, match_type):\n",
    "    \"\"\"Update existing track\"\"\"\n",
    "    global tracks, appearance_features, frame_count\n",
    "\n",
    "    tracks[track_id]['bbox'] = detection\n",
    "    tracks[track_id]['keypoints'] = keypoints\n",
    "    tracks[track_id]['missing_frames'] = 0\n",
    "    tracks[track_id]['last_seen'] = frame_count\n",
    "    tracks[track_id]['match_type'] = match_type\n",
    "\n",
    "    # Update appearance features with moving average\n",
    "    alpha = 0.8  # Weight for old features\n",
    "    old_features = appearance_features[track_id]\n",
    "    appearance_features[track_id] = alpha * old_features + (1 - alpha) * new_features\n",
    "    # Normalize\n",
    "    norm = np.linalg.norm(appearance_features[track_id])\n",
    "    if norm > 0:\n",
    "        appearance_features[track_id] /= norm\n",
    "\n",
    "def remove_old_tracks():\n",
    "    \"\"\"Remove tracks that are too old\"\"\"\n",
    "    global tracks, appearance_features\n",
    "\n",
    "    tracks_to_remove = []\n",
    "    for track_id, track_info in tracks.items():\n",
    "        if track_info['missing_frames'] >= MAX_MISSING_FRAMES:\n",
    "            tracks_to_remove.append(track_id)\n",
    "\n",
    "    for track_id in tracks_to_remove:\n",
    "        del tracks[track_id]\n",
    "        if track_id in appearance_features:\n",
    "            del appearance_features[track_id]\n",
    "\n",
    "def hierarchical_matching(detections, keypoints_list, detection_features, image):\n",
    "    \"\"\"\n",
    "    Hierarchical matching: IoU first, then appearance as fallback\n",
    "    Returns: list of (detection_idx, track_id, match_type) tuples\n",
    "    \"\"\"\n",
    "    global tracks, frame_count\n",
    "\n",
    "    # Get active tracks\n",
    "    active_track_ids = []\n",
    "    for track_id, track_info in tracks.items():\n",
    "        if track_info['missing_frames'] < MAX_MISSING_FRAMES:\n",
    "            active_track_ids.append(track_id)\n",
    "\n",
    "    if not active_track_ids:\n",
    "        return []  # No tracks to match\n",
    "\n",
    "    matches = []\n",
    "    unmatched_detections = set(range(len(detections)))\n",
    "    unmatched_tracks = set(active_track_ids)\n",
    "\n",
    "\n",
    "    # STAGE 1: IoU-based matching for high overlap cases\n",
    "    iou_cost_matrix = np.full((len(detections), len(active_track_ids)), np.inf)\n",
    "\n",
    "    for i, detection in enumerate(detections):\n",
    "        for j, track_id in enumerate(active_track_ids):\n",
    "            track_bbox = tracks[track_id]['bbox']\n",
    "            iou = calculate_iou(detection[:4], track_bbox[:4])\n",
    "\n",
    "            # Only consider if IoU is reasonable\n",
    "            if iou >= MIN_IOU_FOR_CONSIDERATION:\n",
    "                # Cost is inverse of IoU (lower IoU = higher cost)\n",
    "                iou_cost_matrix[i, j] = 1.0 - iou\n",
    "\n",
    "    # Hungarian assignment for IoU matching\n",
    "    if np.any(np.isfinite(iou_cost_matrix)):\n",
    "        row_indices, col_indices = linear_sum_assignment(iou_cost_matrix)\n",
    "\n",
    "        for i, j in zip(row_indices, col_indices):\n",
    "            if iou_cost_matrix[i, j] < (1.0 - IOU_MATCH_THRESHOLD):  # IoU > threshold\n",
    "                track_id = active_track_ids[j]\n",
    "                matches.append((i, track_id, 'iou'))\n",
    "                unmatched_detections.discard(i)\n",
    "                unmatched_tracks.discard(track_id)\n",
    "\n",
    "\n",
    "    # STAGE 2: Appearance-based matching for remaining detections and recently lost tracks\n",
    "    if unmatched_detections and unmatched_tracks:\n",
    "\n",
    "        # Filter tracks for appearance matching (recently seen tracks only)\n",
    "        appearance_track_ids = []\n",
    "        for track_id in unmatched_tracks:\n",
    "            track_info = tracks[track_id]\n",
    "            if track_info['missing_frames'] <= APPEARANCE_MATCH_FRAMES:\n",
    "                appearance_track_ids.append(track_id)\n",
    "\n",
    "        if appearance_track_ids:\n",
    "            appearance_cost_matrix = np.full((len(detections), len(appearance_track_ids)), np.inf)\n",
    "\n",
    "            for i in unmatched_detections:\n",
    "                detection = detections[i]\n",
    "                det_features = detection_features[i]\n",
    "\n",
    "                for j, track_id in enumerate(appearance_track_ids):\n",
    "                    track_bbox = tracks[track_id]['bbox']\n",
    "                    track_features = appearance_features[track_id]\n",
    "\n",
    "                    # Check if distance is reasonable for appearance matching\n",
    "                    distance = calculate_distance(detection[:4], track_bbox[:4])\n",
    "                    if distance <= MAX_DISTANCE_FOR_APPEARANCE:\n",
    "                        # Calculate appearance similarity\n",
    "                        similarity = calculate_appearance_similarity(det_features, track_features)\n",
    "\n",
    "                        if similarity >= APPEARANCE_THRESHOLD:\n",
    "                            # Cost combines distance and appearance (lower = better)\n",
    "                            distance_cost = distance / MAX_DISTANCE_FOR_APPEARANCE  # normalize 0-1\n",
    "                            appearance_cost = 1.0 - similarity  # convert similarity to cost\n",
    "                            combined_cost = 0.3 * distance_cost + 0.7 * appearance_cost\n",
    "                            appearance_cost_matrix[i, j] = combined_cost\n",
    "\n",
    "            # Hungarian assignment for appearance matching\n",
    "            if np.any(np.isfinite(appearance_cost_matrix)):\n",
    "                remaining_detections = list(unmatched_detections)\n",
    "                app_row_indices, app_col_indices = linear_sum_assignment(\n",
    "                    appearance_cost_matrix[remaining_detections, :]\n",
    "                )\n",
    "\n",
    "                for row_idx, col_idx in zip(app_row_indices, app_col_indices):\n",
    "                    detection_idx = remaining_detections[row_idx]\n",
    "                    track_id = appearance_track_ids[col_idx]\n",
    "                    cost = appearance_cost_matrix[detection_idx, col_idx]\n",
    "\n",
    "                    if cost < 0.5:  # Good enough appearance match\n",
    "                        matches.append((detection_idx, track_id, 'appearance'))\n",
    "                        unmatched_detections.discard(detection_idx)\n",
    "                        unmatched_tracks.discard(track_id)\n",
    "                        similarity = 1.0 - (cost * 0.7 / 0.7)  # recover similarity from cost\n",
    "\n",
    "    return matches\n",
    "\n",
    "def update_tracks(detections, keypoints_list, image, dinov3_pipeline):\n",
    "    \"\"\" tracking function with hierarchical matching\"\"\"\n",
    "    global tracks, frame_count\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "    # Filter detections\n",
    "    detections, keypoints_list = filter_detections(detections, keypoints_list)\n",
    "\n",
    "    if not detections:\n",
    "        # No detections - increment missing frames for all tracks\n",
    "        for track_id in tracks:\n",
    "            tracks[track_id]['missing_frames'] += 1\n",
    "        remove_old_tracks()\n",
    "        return\n",
    "\n",
    "\n",
    "    '''detection_features = []\n",
    "    for i, detection in enumerate(detections):\n",
    "        features = extract_appearance_features(image, detection, dinov3_pipeline)\n",
    "        detection_features.append(features)'''\n",
    "    detection_features = extract_appearance_features(image, detections, dinov3_pipeline)\n",
    "\n",
    "    # Hierarchical matching\n",
    "    matches = hierarchical_matching(detections, keypoints_list, detection_features, image)\n",
    "\n",
    "    # Update matched tracks\n",
    "    matched_detection_indices = set()\n",
    "    matched_track_ids = set()\n",
    "\n",
    "    for detection_idx, track_id, match_type in matches:\n",
    "        update_track(track_id, detections[detection_idx], keypoints_list[detection_idx],\n",
    "                    detection_features[detection_idx], match_type)\n",
    "        matched_detection_indices.add(detection_idx)\n",
    "        matched_track_ids.add(track_id)\n",
    "\n",
    "    # Create new tracks for unmatched detections\n",
    "    for i, detection in enumerate(detections):\n",
    "        if i not in matched_detection_indices:\n",
    "            create_new_track(detection, keypoints_list[i], detection_features[i])\n",
    "\n",
    "    # Increment missing frames for unmatched tracks\n",
    "    for track_id, track_info in tracks.items():\n",
    "        if track_id not in matched_track_ids and track_info['missing_frames'] < MAX_MISSING_FRAMES:\n",
    "            tracks[track_id]['missing_frames'] += 1\n",
    "\n",
    "    # Clean up old tracks\n",
    "    remove_old_tracks()\n",
    "\n",
    "    # summary\n",
    "    #iou_matches = len([m for m in matches if m[2] == 'iou'])\n",
    "    #appearance_matches = len([m for m in matches if m[2] == 'appearance'])\n",
    "    #new_tracks = len(detections) - len(matched_detection_indices)\n",
    "    #print(f\"  Matching summary: {iou_matches} IoU, {appearance_matches} appearance, {new_tracks} new tracks\")\n",
    "\n",
    "def draw_pose(img, keypoints, color):\n",
    "    \"\"\"Draw pose keypoints and skeleton\"\"\"\n",
    "    if keypoints is None:\n",
    "        return img\n",
    "\n",
    "    # Draw keypoints\n",
    "    for i, (x, y, conf) in enumerate(keypoints):\n",
    "        if conf > 0.3:\n",
    "            cv2.circle(img, (int(x), int(y)), 4, color, -1)\n",
    "\n",
    "    # Draw skeleton\n",
    "    for connection in SKELETON:\n",
    "        pt1_idx, pt2_idx = connection[0] - 1, connection[1] - 1\n",
    "\n",
    "        if (pt1_idx < len(keypoints) and pt2_idx < len(keypoints) and\n",
    "            keypoints[pt1_idx][2] > 0.3 and keypoints[pt2_idx][2] > 0.3):\n",
    "\n",
    "            pt1 = (int(keypoints[pt1_idx][0]), int(keypoints[pt1_idx][1]))\n",
    "            pt2 = (int(keypoints[pt2_idx][0]), int(keypoints[pt2_idx][1]))\n",
    "            cv2.line(img, pt1, pt2, color, 2)\n",
    "\n",
    "    return img\n",
    "\n",
    "def draw_tracks(img, tracks):\n",
    "    \"\"\"Draw all active tracks with match type indicator\"\"\"\n",
    "    active_tracks = {tid: info for tid, info in tracks.items()\n",
    "                    if info['missing_frames'] <= 3}\n",
    "\n",
    "    for track_id, track_info in active_tracks.items():\n",
    "        # color\n",
    "        color = COLORS[track_id % len(COLORS)]\n",
    "\n",
    "        # Draw bounding box\n",
    "        bbox = track_info['bbox']\n",
    "        x1, y1, x2, y2 = map(int, bbox[:4])\n",
    "\n",
    "        # Different line styles for different match types\n",
    "        thickness = 3 if track_info.get('match_type') == 'iou' else 2\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color, thickness)\n",
    "\n",
    "        # Draw track ID with match type\n",
    "        match_type = track_info.get('match_type', 'new')\n",
    "        label = f'ID:{track_id} ({match_type})'\n",
    "        cv2.putText(img, label, (x1, y1-10),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "        # Draw pose\n",
    "        img = draw_pose(img, track_info['keypoints'], color)\n",
    "\n",
    "    return img\n",
    "\n",
    "def process_video(video_path, output_path):\n",
    "    \"\"\"Process a single video\"\"\"\n",
    "    print(f\"Processing: {video_path}\")\n",
    "\n",
    "    global tracks, next_id, frame_count, appearance_features\n",
    "    tracks = {}\n",
    "    next_id = 1\n",
    "    frame_count = 0\n",
    "    appearance_features = {}\n",
    "\n",
    "    yolo_model = YOLO('yolov8n-pose.pt')\n",
    "\n",
    "    dinov3_pipeline = load_dinov3_model()\n",
    "\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Cannot open video {video_path}\")\n",
    "        return\n",
    "\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        results = yolo_model(frame, verbose=False)\n",
    "        detections = []\n",
    "        keypoints_list = []\n",
    "\n",
    "        if results[0].boxes is not None:\n",
    "            boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "            scores = results[0].boxes.conf.cpu().numpy()\n",
    "            classes = results[0].boxes.cls.cpu().numpy()\n",
    "\n",
    "            # Filter for person class (class 0)\n",
    "            person_indices = classes == 0\n",
    "            boxes = boxes[person_indices]\n",
    "            scores = scores[person_indices]\n",
    "\n",
    "            # Get keypoints if available\n",
    "            if results[0].keypoints is not None:\n",
    "                keypoints = results[0].keypoints.xy.cpu().numpy()[person_indices]\n",
    "                keypoints_conf = results[0].keypoints.conf.cpu().numpy()[person_indices]\n",
    "\n",
    "                for i in range(len(boxes)):\n",
    "                    x1, y1, x2, y2 = boxes[i]\n",
    "                    conf = scores[i]\n",
    "                    detections.append([x1, y1, x2, y2, conf])\n",
    "\n",
    "                    # Combine keypoint coordinates with confidence\n",
    "                    kpts = []\n",
    "                    for j in range(len(keypoints[i])):\n",
    "                        x, y = keypoints[i][j]\n",
    "                        c = keypoints_conf[i][j]\n",
    "                        kpts.append([x, y, c])\n",
    "                    keypoints_list.append(kpts)\n",
    "            else:\n",
    "                # No keypoints available\n",
    "                for i in range(len(boxes)):\n",
    "                    x1, y1, x2, y2 = boxes[i]\n",
    "                    conf = scores[i]\n",
    "                    detections.append([x1, y1, x2, y2, conf])\n",
    "                    keypoints_list.append(None)\n",
    "\n",
    "        # Update tracking with hierarchical matching\n",
    "        update_tracks(detections, keypoints_list, frame, dinov3_pipeline)\n",
    "\n",
    "        #results\n",
    "        output_frame = draw_tracks(frame.copy(), tracks)\n",
    "\n",
    "        #frame info\n",
    "        active_count = len([t for t in tracks.values() if t['missing_frames'] <= 3])\n",
    "        iou_count = len([t for t in tracks.values() if t.get('match_type') == 'iou'])\n",
    "        app_count = len([t for t in tracks.values() if t.get('match_type') == 'appearance'])\n",
    "\n",
    "        info_text = f\"Frame: {frame_count}/{total_frames} | Active: {active_count} | IoU: {iou_count} | App: {app_count}\"\n",
    "        cv2.putText(output_frame, info_text, (10, 30),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "        writer.write(output_frame)\n",
    "\n",
    "        if frame_count % 50 == 0:\n",
    "            print(f\"Processed {frame_count}/{total_frames} frames\")\n",
    "\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "\n",
    "    print(f\"Video saved to: {output_path}\")\n",
    "    print(f\"Total unique people tracked: {next_id - 1}\")\n",
    "\n",
    "\n",
    "    tracking_data = {\n",
    "        'video_file': os.path.basename(video_path),\n",
    "        'tracks': tracks,\n",
    "        'appearance_features': appearance_features,\n",
    "        'total_frames': frame_count,\n",
    "        'fps': fps,\n",
    "        'parameters': {\n",
    "            'iou_threshold': IOU_MATCH_THRESHOLD,\n",
    "            'appearance_threshold': APPEARANCE_THRESHOLD,\n",
    "            'max_distance_for_appearance': MAX_DISTANCE_FOR_APPEARANCE\n",
    "        }\n",
    "    }\n",
    "\n",
    "    data_file = output_path.replace('.mp4', '_hierarchical_tracking_data.pkl')\n",
    "    with open(data_file, 'wb') as f:\n",
    "        pickle.dump(tracking_data, f)\n",
    "    print(f\"Tracking data saved to: {data_file}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    input_folder = '/kaggle/input/360p-video/you_video360p'\n",
    "    output_folder = '/kaggle/working/hierarchical_tracking_output'\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    video_extensions = ['.mp4', '.avi', '.mov', '.mkv']\n",
    "    video_files = []\n",
    "\n",
    "    if os.path.exists(input_folder):\n",
    "        for file in os.listdir(input_folder):\n",
    "            if any(file.lower().endswith(ext) for ext in video_extensions):\n",
    "                video_files.append(file)\n",
    "    else:\n",
    "        print(f\"Input folder not found: {input_folder}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(video_files)} video(s) to process\")\n",
    "\n",
    "    for video_file in video_files:\n",
    "        print(f\"\\n Processing: {video_file}\")\n",
    "\n",
    "        video_path = os.path.join(input_folder, video_file)\n",
    "        output_path = os.path.join(output_folder,\n",
    "                                  video_file.replace('.mp4', '_hierarchical_tracked.mp4'))\n",
    "\n",
    "        process_video(video_path, output_path)\n",
    "\n",
    "\n",
    "    print(\"\\nAll videos processed \")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4b0748",
   "metadata": {},
   "source": [
    "# cost - combained cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9db2c97",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "tracks = {}           # Store all tracks: {track_id: track_info}\n",
    "next_id = 1          # Next available track ID\n",
    "frame_count = 0      # Current frame number\n",
    "appearance_features = {}  # Store appearance features: {track_id: features}\n",
    "\n",
    "\n",
    "MAX_MISSING_FRAMES = 30    # Remove track after this many frames\n",
    "DISTANCE_THRESHOLD = 100   # Max distance for matching\n",
    "MIN_CONFIDENCE = 0.5       # Minimum detection confidence\n",
    "APPEARANCE_THRESHOLD = 0.7 # Minimum appearance similarity\n",
    "\n",
    "\n",
    "COLORS = [\n",
    "    (255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0),\n",
    "    (255, 0, 255), (0, 255, 255), (128, 0, 128), (255, 128, 0)\n",
    "]\n",
    "\n",
    "SKELETON = [\n",
    "    [16, 14], [14, 12], [17, 15], [15, 13], [12, 13],\n",
    "    [6, 12], [7, 13], [6, 7], [6, 8], [7, 9],\n",
    "    [8, 10], [9, 11], [2, 3], [1, 2], [1, 3],\n",
    "    [2, 4], [3, 5], [4, 6], [5, 7]\n",
    "]\n",
    "\n",
    "def load_dinov3_model():\n",
    "\n",
    "    device = 0 if os.system('nvidia-smi') == 0 else -1\n",
    "    # Create the pipeline\n",
    "    dinov3_pipeline = pipeline(\n",
    "        \"image-feature-extraction\",\n",
    "        model=\"facebook/dinov3-vith16plus-pretrain-lvd1689m\",\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    return dinov3_pipeline\n",
    "\n",
    "def extract_appearance_features(image, bbox, dinov3_pipeline):\n",
    "    \"\"\"Extract appearance features\"\"\"\n",
    "    x1, y1, x2, y2 = map(int, bbox[:4])\n",
    "\n",
    "    # Make sure bbox is valid\n",
    "    h, w = image.shape[:2]\n",
    "    x1 = max(0, min(x1, w-1))\n",
    "    y1 = max(0, min(y1, h-1))\n",
    "    x2 = max(x1+1, min(x2, w))\n",
    "    y2 = max(y1+1, min(y2, h))\n",
    "\n",
    "    # Extract person crop\n",
    "    person_crop = image[y1:y2, x1:x2]\n",
    "\n",
    "    # Convert BGR to RGB\n",
    "    person_crop_rgb = cv2.cvtColor(person_crop, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Convert to PIL Image\n",
    "    pil_image = Image.fromarray(person_crop_rgb)\n",
    "\n",
    "    # Resize to standard size for better features\n",
    "    pil_image = pil_image.resize((224, 224))\n",
    "\n",
    "    # Extract features using the transformers pipeline\n",
    "    features = dinov3_pipeline([pil_image])[0]\n",
    "\n",
    "    # Convert to numpy array and flatten\n",
    "    features = np.array(features).flatten()\n",
    "\n",
    "    # Normalize the features\n",
    "    norm = np.linalg.norm(features)\n",
    "    if norm > 0:\n",
    "        features = features / norm\n",
    "\n",
    "    return features\n",
    "\n",
    "def calculate_distance(bbox1, bbox2):\n",
    "    \"\"\"Calculate distance between centers of two bounding boxes\"\"\"\n",
    "    center1 = [(bbox1[0] + bbox1[2])/2, (bbox1[1] + bbox1[3])/2]\n",
    "    center2 = [(bbox2[0] + bbox2[2])/2, (bbox2[1] + bbox2[3])/2]\n",
    "    return np.sqrt((center1[0] - center2[0])**2 + (center1[1] - center2[1])**2)\n",
    "\n",
    "def calculate_iou(bbox1, bbox2):\n",
    "    \"\"\"Calculate Intersection over Union\"\"\"\n",
    "    x1 = max(bbox1[0], bbox2[0])\n",
    "    y1 = max(bbox1[1], bbox2[1])\n",
    "    x2 = min(bbox1[2], bbox2[2])\n",
    "    y2 = min(bbox1[3], bbox2[3])\n",
    "\n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return 0.0\n",
    "\n",
    "    intersection = (x2 - x1) * (y2 - y1)\n",
    "    area1 = (bbox1[2] - bbox1[0]) * (bbox1[3] - bbox1[1])\n",
    "    area2 = (bbox2[2] - bbox2[0]) * (bbox2[3] - bbox2[1])\n",
    "    union = area1 + area2 - intersection\n",
    "\n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "def calculate_appearance_similarity(features1, features2):\n",
    "    \"\"\"Calculate cosine similarity between appearance features\"\"\"\n",
    "    # Cosine similarity\n",
    "    dot_product = np.dot(features1, features2)\n",
    "    norm1 = np.linalg.norm(features1)\n",
    "    norm2 = np.linalg.norm(features2)\n",
    "\n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0.0\n",
    "\n",
    "    similarity = dot_product / (norm1 * norm2)\n",
    "    return max(0, similarity)  # Clip negative values\n",
    "\n",
    "def filter_detections(detections, keypoints_list):\n",
    "    \"\"\"Keep only good quality detections\"\"\"\n",
    "    if not detections:\n",
    "        return [], []\n",
    "\n",
    "    filtered_detections = []\n",
    "    filtered_keypoints = []\n",
    "\n",
    "    for i, detection in enumerate(detections):\n",
    "        x1, y1, x2, y2, conf = detection\n",
    "\n",
    "        # Check confidence and box size\n",
    "        width = x2 - x1\n",
    "        height = y2 - y1\n",
    "        area = width * height\n",
    "        aspect_ratio = width / height if height > 0 else 0\n",
    "\n",
    "        # Filter criteria\n",
    "        if (conf > MIN_CONFIDENCE and\n",
    "            area > 2000 and\n",
    "            0.2 < aspect_ratio < 5.0 and\n",
    "            width > 50 and height > 100):\n",
    "\n",
    "            filtered_detections.append(detection)\n",
    "            if i < len(keypoints_list):\n",
    "                filtered_keypoints.append(keypoints_list[i])\n",
    "            else:\n",
    "                filtered_keypoints.append(None)\n",
    "\n",
    "    return filtered_detections, filtered_keypoints\n",
    "\n",
    "def create_new_track(detection, keypoints, features):\n",
    "    \"\"\"Create a new track\"\"\"\n",
    "    global next_id, frame_count, tracks, appearance_features\n",
    "\n",
    "    track_id = next_id\n",
    "    next_id += 1\n",
    "\n",
    "    tracks[track_id] = {\n",
    "        'bbox': detection,\n",
    "        'keypoints': keypoints,\n",
    "        'missing_frames': 0,\n",
    "        'last_seen': frame_count,\n",
    "        'created_at': frame_count\n",
    "    }\n",
    "\n",
    "    # Store appearance features\n",
    "    appearance_features[track_id] = features\n",
    "\n",
    "    return track_id\n",
    "\n",
    "def update_track(track_id, detection, keypoints, new_features):\n",
    "    \"\"\"Update existing track\"\"\"\n",
    "    global tracks, appearance_features, frame_count\n",
    "\n",
    "    tracks[track_id]['bbox'] = detection\n",
    "    tracks[track_id]['keypoints'] = keypoints\n",
    "    tracks[track_id]['missing_frames'] = 0\n",
    "    tracks[track_id]['last_seen'] = frame_count\n",
    "\n",
    "    # Update appearance features with moving average\n",
    "    alpha = 0.8  # Weight for old features\n",
    "    old_features = appearance_features[track_id]\n",
    "    appearance_features[track_id] = alpha * old_features + (1 - alpha) * new_features\n",
    "    # Normalize\n",
    "    norm = np.linalg.norm(appearance_features[track_id])\n",
    "    if norm > 0:\n",
    "        appearance_features[track_id] /= norm\n",
    "\n",
    "def remove_old_tracks():\n",
    "    \"\"\"Remove tracks that are too old\"\"\"\n",
    "    global tracks, appearance_features\n",
    "\n",
    "    tracks_to_remove = []\n",
    "    for track_id, track_info in tracks.items():\n",
    "        if track_info['missing_frames'] >= MAX_MISSING_FRAMES:\n",
    "            tracks_to_remove.append(track_id)\n",
    "\n",
    "    for track_id in tracks_to_remove:\n",
    "        del tracks[track_id]\n",
    "        if track_id in appearance_features:\n",
    "            del appearance_features[track_id]\n",
    "\n",
    "def update_tracks(detections, keypoints_list, image, dinov3_pipeline):\n",
    "    \"\"\"Main tracking function\"\"\"\n",
    "    global tracks, frame_count\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "    # Filter detections\n",
    "    detections, keypoints_list = filter_detections(detections, keypoints_list)\n",
    "\n",
    "    if not detections:\n",
    "        # No detections - increment missing frames for all tracks\n",
    "        for track_id in tracks:\n",
    "            tracks[track_id]['missing_frames'] += 1\n",
    "        remove_old_tracks()\n",
    "        return\n",
    "\n",
    "    # Extract appearance features for all detections using DINOv3 pipeline\n",
    "    print(f\"Extracting features for {len(detections)} detections...\")\n",
    "    detection_features = []\n",
    "    for i, detection in enumerate(detections):\n",
    "        features = extract_appearance_features(image, detection, dinov3_pipeline)\n",
    "        detection_features.append(features)\n",
    "        if (i + 1) % 5 == 0:  # Progress update every 5 detections\n",
    "            print(f\"  Processed {i+1}/{len(detections)} detections\")\n",
    "\n",
    "    # Get active tracks\n",
    "    active_track_ids = []\n",
    "    for track_id, track_info in tracks.items():\n",
    "        if track_info['missing_frames'] < MAX_MISSING_FRAMES:\n",
    "            active_track_ids.append(track_id)\n",
    "\n",
    "    if not active_track_ids:\n",
    "        # No active tracks - create new tracks for all detections\n",
    "        for i, detection in enumerate(detections):\n",
    "            create_new_track(detection, keypoints_list[i], detection_features[i])\n",
    "        return\n",
    "\n",
    "    # Create cost matrix for matching\n",
    "    num_detections = len(detections)\n",
    "    num_tracks = len(active_track_ids)\n",
    "    cost_matrix = np.full((num_detections, num_tracks), 1000.0)\n",
    "\n",
    "    for i, detection in enumerate(detections):\n",
    "        for j, track_id in enumerate(active_track_ids):\n",
    "            track_bbox = tracks[track_id]['bbox']\n",
    "\n",
    "            # Calculate costs\n",
    "            distance = calculate_distance(detection[:4], track_bbox[:4])\n",
    "            iou = calculate_iou(detection[:4], track_bbox[:4])\n",
    "\n",
    "            # Appearance similarity using DINOv3 features\n",
    "            appearance_sim = calculate_appearance_similarity(\n",
    "                detection_features[i], appearance_features[track_id]\n",
    "            )\n",
    "\n",
    "            # Combined cost (lower is better)\n",
    "            if distance < DISTANCE_THRESHOLD and appearance_sim > APPEARANCE_THRESHOLD:\n",
    "                # Good match - low cost\n",
    "                cost = distance - (iou * 50) - (appearance_sim * 100)\n",
    "                cost_matrix[i, j] = max(0, cost)\n",
    "            elif distance < DISTANCE_THRESHOLD:\n",
    "                # Decent spatial match\n",
    "                cost = distance - (iou * 30)\n",
    "                cost_matrix[i, j] = max(0, cost)\n",
    "\n",
    "    # Hungarian assignment\n",
    "    if cost_matrix.size > 0:\n",
    "        row_indices, col_indices = linear_sum_assignment(cost_matrix)\n",
    "\n",
    "        assigned_detections = set()\n",
    "        assigned_tracks = set()\n",
    "\n",
    "        # Update matched tracks\n",
    "        for i, j in zip(row_indices, col_indices):\n",
    "            if cost_matrix[i, j] < 300:  # Good enough match\n",
    "                track_id = active_track_ids[j]\n",
    "                update_track(track_id, detections[i], keypoints_list[i], detection_features[i])\n",
    "                assigned_detections.add(i)\n",
    "                assigned_tracks.add(track_id)\n",
    "\n",
    "        # Create new tracks for unassigned detections\n",
    "        for i, detection in enumerate(detections):\n",
    "            if i not in assigned_detections:\n",
    "                create_new_track(detection, keypoints_list[i], detection_features[i])\n",
    "\n",
    "        # Increment missing frames for unassigned tracks\n",
    "        for track_id in active_track_ids:\n",
    "            if track_id not in assigned_tracks:\n",
    "                tracks[track_id]['missing_frames'] += 1\n",
    "\n",
    "    # Clean up old tracks\n",
    "    remove_old_tracks()\n",
    "\n",
    "def draw_pose(img, keypoints, color):\n",
    "    \"\"\"Draw pose keypoints and skeleton\"\"\"\n",
    "    if keypoints is None:\n",
    "        return img\n",
    "\n",
    "    # Draw keypoints\n",
    "    for i, (x, y, conf) in enumerate(keypoints):\n",
    "        if conf > 0.3:\n",
    "            cv2.circle(img, (int(x), int(y)), 4, color, -1)\n",
    "\n",
    "    # Draw skeleton\n",
    "    for connection in SKELETON:\n",
    "        pt1_idx, pt2_idx = connection[0] - 1, connection[1] - 1\n",
    "\n",
    "        if (pt1_idx < len(keypoints) and pt2_idx < len(keypoints) and\n",
    "            keypoints[pt1_idx][2] > 0.3 and keypoints[pt2_idx][2] > 0.3):\n",
    "\n",
    "            pt1 = (int(keypoints[pt1_idx][0]), int(keypoints[pt1_idx][1]))\n",
    "            pt2 = (int(keypoints[pt2_idx][0]), int(keypoints[pt2_idx][1]))\n",
    "            cv2.line(img, pt1, pt2, color, 2)\n",
    "\n",
    "    return img\n",
    "\n",
    "def draw_tracks(img, tracks):\n",
    "    \"\"\"Draw all active tracks\"\"\"\n",
    "    active_tracks = {tid: info for tid, info in tracks.items()\n",
    "                    if info['missing_frames'] <= 3}\n",
    "\n",
    "    for track_id, track_info in active_tracks.items():\n",
    "        # Get color\n",
    "        color = COLORS[track_id % len(COLORS)]\n",
    "\n",
    "        # Draw bounding box\n",
    "        bbox = track_info['bbox']\n",
    "        x1, y1, x2, y2 = map(int, bbox[:4])\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
    "\n",
    "        # Draw track ID\n",
    "        cv2.putText(img, f'ID: {track_id}', (x1, y1-10),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "\n",
    "        # Draw pose\n",
    "        img = draw_pose(img, track_info['keypoints'], color)\n",
    "\n",
    "    return img\n",
    "\n",
    "def process_video(video_path, output_path):\n",
    "    \"\"\"Process a single video\"\"\"\n",
    "    print(f\"Processing: {video_path}\")\n",
    "\n",
    "\n",
    "    global tracks, next_id, frame_count, appearance_features\n",
    "    tracks = {}\n",
    "    next_id = 1\n",
    "    frame_count = 0\n",
    "    appearance_features = {}\n",
    "\n",
    "\n",
    "    yolo_model = YOLO('yolov8n-pose.pt')\n",
    "\n",
    "\n",
    "    dinov3_pipeline = load_dinov3_model()\n",
    "\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Cannot open video {video_path}\")\n",
    "        return\n",
    "\n",
    "\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    print(f\"Processing {total_frames} frames with DINOv3 transformers pipeline...\")\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "\n",
    "        results = yolo_model(frame, verbose=False)\n",
    "\n",
    "        # Extract person detections and poses\n",
    "        detections = []\n",
    "        keypoints_list = []\n",
    "\n",
    "        if results[0].boxes is not None:\n",
    "            boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "            scores = results[0].boxes.conf.cpu().numpy()\n",
    "            classes = results[0].boxes.cls.cpu().numpy()\n",
    "\n",
    "            # Filter for person class (class 0)\n",
    "            person_indices = classes == 0\n",
    "            boxes = boxes[person_indices]\n",
    "            scores = scores[person_indices]\n",
    "\n",
    "            # Get keypoints if available\n",
    "            if results[0].keypoints is not None:\n",
    "                keypoints = results[0].keypoints.xy.cpu().numpy()[person_indices]\n",
    "                keypoints_conf = results[0].keypoints.conf.cpu().numpy()[person_indices]\n",
    "\n",
    "                for i in range(len(boxes)):\n",
    "                    x1, y1, x2, y2 = boxes[i]\n",
    "                    conf = scores[i]\n",
    "                    detections.append([x1, y1, x2, y2, conf])\n",
    "\n",
    "                    # Combine keypoint coordinates with confidence\n",
    "                    kpts = []\n",
    "                    for j in range(len(keypoints[i])):\n",
    "                        x, y = keypoints[i][j]\n",
    "                        c = keypoints_conf[i][j]\n",
    "                        kpts.append([x, y, c])\n",
    "                    keypoints_list.append(kpts)\n",
    "            else:\n",
    "                # No keypoints available\n",
    "                for i in range(len(boxes)):\n",
    "                    x1, y1, x2, y2 = boxes[i]\n",
    "                    conf = scores[i]\n",
    "                    detections.append([x1, y1, x2, y2, conf])\n",
    "                    keypoints_list.append(None)\n",
    "\n",
    "\n",
    "        update_tracks(detections, keypoints_list, frame, dinov3_pipeline)\n",
    "\n",
    "        # Draw results\n",
    "        output_frame = draw_tracks(frame.copy(), tracks)\n",
    "\n",
    "        #  frame info\n",
    "        active_count = len([t for t in tracks.values() if t['missing_frames'] <= 3])\n",
    "        info_text = f\"Frame: {frame_count}/{total_frames} | Active: {active_count} | Total: {next_id-1}\"\n",
    "        cv2.putText(output_frame, info_text, (10, 30),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "        writer.write(output_frame)\n",
    "\n",
    "        if frame_count % 50 == 0:\n",
    "            print(f\"Processed {frame_count}/{total_frames} frames\")\n",
    "\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "\n",
    "    print(f\"Video saved to: {output_path}\")\n",
    "    print(f\"Total unique people tracked: {next_id - 1}\")\n",
    "\n",
    "\n",
    "    tracking_data = {\n",
    "        'video_file': os.path.basename(video_path),\n",
    "        'tracks': tracks,\n",
    "        'appearance_features': appearance_features,\n",
    "        'total_frames': frame_count,\n",
    "        'fps': fps\n",
    "    }\n",
    "\n",
    "    data_file = output_path.replace('.mp4', '_tracking_data.pkl')\n",
    "    with open(data_file, 'wb') as f:\n",
    "        pickle.dump(tracking_data, f)\n",
    "    print(f\"Tracking data saved to: {data_file}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "\n",
    "    input_folder = '/kaggle/input/360p-video/you_video360p'\n",
    "    output_folder = '/kaggle/working/dinov3_tracking_output'\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    video_extensions = ['.mp4', '.avi', '.mov', '.mkv']\n",
    "    video_files = []\n",
    "\n",
    "    if os.path.exists(input_folder):\n",
    "        for file in os.listdir(input_folder):\n",
    "            if any(file.lower().endswith(ext) for ext in video_extensions):\n",
    "                video_files.append(file)\n",
    "    else:\n",
    "        print(f\"Input folder not found: {input_folder}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(video_files)} video(s) to process\")\n",
    "\n",
    "\n",
    "    for video_file in video_files:\n",
    "        print(f\"\\n=== Processing: {video_file} ===\")\n",
    "\n",
    "        video_path = os.path.join(input_folder, video_file)\n",
    "        output_path = os.path.join(output_folder,\n",
    "                                  video_file.replace('.mp4', '_dinov3_tracked.mp4'))\n",
    "\n",
    "\n",
    "        process_video(video_path, output_path)\n",
    "    print(\"\\nAll videos processed\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
